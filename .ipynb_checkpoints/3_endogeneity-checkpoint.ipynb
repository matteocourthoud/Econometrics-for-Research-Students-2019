{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [Instrumental Variables](#Instrumental-Variables)\n",
    " - [GMM](#GMM)\n",
    " - [Testing Overidentifying Restrictions](#Testing-Overidentifying-Restrictions)\n",
    " - [Many Instrument Robust Estimation](#Many-Instrument-Robust-Estimation)\n",
    " - [Hausman Test](#Hausman-Test)\n",
    " - [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import julia packages \n",
    "using Random, LinearAlgebra, Statistics, StatsBase, Plots, StatsFuns\n",
    "default(size=(2000,1600), legend=false, thickness_scaling=3, linewidth=3)\n",
    "\n",
    "# Set seed\n",
    "Random.seed!(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of recap..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of observations\n",
    "n = 100;\n",
    "\n",
    "# Set the dimension of X\n",
    "k = 2;\n",
    "\n",
    "# Draw a sample of explanatory variables\n",
    "X = rand(n, k);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumental Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that there is **endogeneity** in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\n",
    "\n",
    "The random vector $z_i$ is an **instrumental variable** in the linear regression model if the following conditions are met.\n",
    "\n",
    "- **Exclusion restriction**: the instruments are uncorrelated with the regression error\n",
    "  $$\n",
    "    \\mathbb E_n[z_i \\varepsilon_i] = 0 \n",
    "  $$\n",
    "  almost surely, i.e. with probability $p \\to 1$.\n",
    "- **Rank condition**: no linearly redundant instruments\n",
    "  $$ \n",
    "    \\mathbb E_n[z_i z_i'] \\neq 0 \n",
    "  $$\n",
    "  almost surely, i.e. with probability $p \\to 1$.\n",
    "- **Relevance condition** (need $L > K$):\n",
    "  $$\n",
    "    rank \\ (\\mathbb E_n[z_i x_i']) = K\n",
    "  $$\n",
    "  almost surely, i.e. with probability $p \\to 1$.\n",
    "  \n",
    "Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is **just-identified** if $L = K$ (method: IV) and **over-identified** if $L > K$ (method: 2SLS). \n",
    "\n",
    "Assume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) = dim(x_i)$, then the **instrumental variables (IV)** estimator $\\hat{\\beta} _ {IV}$ is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\hat{\\beta} _ {IV} &= \\mathbb E_n[z_i x_i']^{-1} \\mathbb E_n[z_i y_i] = \\\\\n",
    "\t&= \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i x_i\\right)^{-1} \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i y_i\\right) = \\\\\n",
    "\t&= (Z'X)^{-1} (Z'y) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) > dim(x_i)$, then the **two-stage-least squares (2SLS)** estimator $\\hat{\\beta} _ {2SLS}$ is given by\n",
    "$$\n",
    "\t\\hat{\\beta} _ {2SLS} =  \\Big( X'Z (Z'Z)^{-1} Z'X \\Big)^{-1} \\Big( X'Z (Z'Z)^{-1} Z'y \\Big)\n",
    "$$\n",
    "Where $\\hat{x}_i$ is the predicted $x_i$ from the **first stage** regression of $x_i$ on $z_i$. This is equivalent of the IV estimator using $\\hat{x}_i$ as an instrument for $x_i$. \n",
    "\n",
    "> On the algebra of 2SLS:\n",
    ">\n",
    "- The estimator is called \\textit{two-stage-least squares} since it can be rewritten as an IV estimator that uses $\\hat{X}$ as instrument:\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t\t\\hat{\\beta} _ {\\text{2SLS}} &= \\Big( X'Z (Z'Z)^{-1} Z'X \\Big)^{-1} \\Big( X'Z (Z'Z)^{-1} Z'y \\Big) = \\\\\n",
    "\t\t&= (\\hat{X}' X)^{-1} \\hat{X}' y = \\\\\n",
    "\t\t&= \\mathbb E_n[\\hat{x}_i x_i']^{-1} \\mathbb E_n[\\hat{x}_i y_i] \n",
    "\t\\end{aligned}\n",
    "\t$$\n",
    "- Moreover it can be rewritten as\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t\t\\hat{\\beta} _ {\\text{2SLS}} &= (\\hat{X}' X)^{-1} \\hat{X}' y = \\\\\n",
    "\t\t&= (X' P_Z X)^{-1} X' P_Z y = \\\\\n",
    "\t\t&= (X' P_Z P_Z X)^{-1} X' P_Z y = \\\\\n",
    "\t\t&= (\\hat{X}' \\hat{X})^{-1} \\hat{X}' y = \\\\\n",
    "\t\t&= \\mathbb E_n [\\hat{x}_i \\hat{x}_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \n",
    "\t\\end{aligned}\n",
    "\t$$\n",
    "- How to the test the relevance condition? Rule of thumb: $F$-test in the first stage $>10$ (joint test on $z_i$). \n",
    "  **Problem**: as $n \\to \\infty$, with finite $L$, $F \\to \\infty$ (bad rule of thumb). \n",
    "\n",
    "\n",
    "\n",
    "**Theorem**:\n",
    "\tIf $K=L$, $\\hat{\\beta} _ {\\text{2SLS}} = \\hat{\\beta} _ {\\text{IV}}$. \n",
    "\n",
    "**Proof**:\n",
    "If $K=L$, $X'Z$ and $Z'X$ are squared matrices and, by the relevance condition, non-singular (invertible).\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\hat{\\beta} _ {\\text{2SLS}} &= \\Big( X'Z (Z'Z)^{-1} Z'X \\Big)^{-1} \\Big( X'Z (Z'Z)^{-1} Z'y \\Big) = \\\\\n",
    "\t&= (Z'X)^{-1} (Z'Z) (X'Z)^{-1} X'Z (Z'Z)^{-1} Z'y = \\\\\n",
    "\t&= (Z'X)^{-1} (Z'Z) (Z'Z)^{-1} Z'y = \\\\\n",
    "\t&= (Z'X)^{-1} (Z'y) = \\\\\n",
    "\t&= \\hat{\\beta} _ {\\text{IV}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "\n",
    "> **Example** from Hayiashi (2000) page 187: demand and supply simultaneous equations. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& q_i^D(p_i) = \\alpha_0 + \\alpha_1 p_i + u_i \\\\\n",
    "\t& q_i^S(p_i) = \\beta_0 + \\beta_1 p_i + v_i \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "We have an endogeneity problem. To see why, we solve the system of equations for $(p_i, q_i)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& p_i = \\frac{\\beta_0 - \\alpha_0}{\\alpha_1 - \\beta_1} + \\frac{v_i - u_i}{\\alpha_1 - \\beta_1 } \\\\\n",
    "\t& q_i = \\frac{\\alpha_1\\beta_0 - \\alpha_0 \\beta_1}{\\alpha_1 - \\beta_1} + \\frac{\\alpha_1 v_i - \\beta_1 u_i}{\\alpha_1 - \\beta_1 } \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "Then the price variable is not independent from the error term in neither equation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& Cov(p_i, u_i) = - \\frac{Var(u_i)}{\\alpha_1 - \\beta_1 } \\\\\n",
    "\t& Cov(p_i, v_i) = \\frac{Var(v_i)}{\\alpha_1 - \\beta_1 } \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "As a consequence, the two coefficient estimates are not consistent:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& \\hat{\\alpha} _ {1, OLS} \\overset{p}{\\to} \\alpha_1 + \\frac{Cov(p_i, u_i)}{Var(p_i)} \\\\\n",
    "\t& \\hat{\\beta} _ {1, OLS} \\overset{p}{\\to} \\beta_1 + \\frac{Cov(p_i, v_i)}{Var(p_i)} \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "In general, running OLS on $q_i = \\gamma p_i + \\varepsilon_i$ you estimate\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\hat{\\gamma} _ {OLS} &\\overset{p}{\\to} \\frac{Cov(p_i, q_i)}{Var(p_i)} = \\\\\n",
    "  &= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\left( \\frac{Var(v_i) + Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\right)^{-1} = \\\\\n",
    "  &= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{Var(v_i) + Var(u_i)} \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "Which is neither $\\alpha_1$ nor $\\beta_1$ but a variance weighted average of the two.\n",
    ">\n",
    "Suppose we have a supply shifter $z_i$ such that $\\mathbb E[z_i v_i] \\neq 0$ and $\\mathbb E[z_i u_i] = 0$. We combine the second condition and $\\mathbb E[u_i] = 0$ to get a system of 2 equations in 2 unknowns: $\\alpha_0$ and $\\alpha_1$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& \\mathbb E[z_i u_i] = \\mathbb E[ z_i (q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i) ] = 0 \\\\\n",
    "\t& \\mathbb E[u_i] = \\mathbb E[q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i] = 0  \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "We could try to solve for the vector $\\alpha$ that solves\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& \\mathbb E_n[z_i (q_i^D - x_i\\alpha)] = 0 \\\\\n",
    "\t& \\mathbb E_n[z_i q_i^D] -  \\mathbb E_n[z_ix_i\\alpha] = 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "If $\\mathbb E_n[z_ix_i]$ is invertible, we get $\\hat{\\alpha} = \\mathbb E_n[z_ix_i]^{-1} \\mathbb E_n[z_i q^D_i]$ which is indeed the IV estimator of $\\alpha$ using $z_i$ as an instrument for the endogenous variable $p_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch(\"dimensions must match: a has dims (Base.OneTo(100), Base.OneTo(2)), must have singleton at dim 2\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"dimensions must match: a has dims (Base.OneTo(100), Base.OneTo(2)), must have singleton at dim 2\")",
      "",
      "Stacktrace:",
      " [1] promote_shape",
      "   @ ./indices.jl:183 [inlined]",
      " [2] promote_shape(a::Matrix{Float64}, b::Vector{Float64})",
      "   @ Base ./indices.jl:169",
      " [3] +(A::Matrix{Float64}, Bs::Vector{Float64})",
      "   @ Base ./arraymath.jl:45",
      " [4] top-level scope",
      "   @ In[21]:13",
      " [5] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "# Set the dimension of Z\n",
    "l = 3;\n",
    "\n",
    "# Draw instruments\n",
    "Z = rand(n,l);\n",
    "\n",
    "# Correlation matrix for error terms\n",
    "S = [1 0.8; 0.8 1]\n",
    "\n",
    "# Endogenous X\n",
    "gamma = [2 0; 0 -1; -1 3];\n",
    "e = rand(n,2) * Array(cholesky(S));\n",
    "X = Z*gamma + e[:,1];\n",
    "\n",
    "# Calculate Y\n",
    "Y = X*b + e[:,2];\n",
    "\n",
    "# Estimate beta OLS\n",
    "beta_OLS = (X'*X)\\(X'*Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV: l=k=2 instruments\n",
    "Z_IV = Z(:,1:k);\n",
    "beta_IV = (Z_IV'*X)\\(Z_IV'*Y) % = 2.1207, -1.3617\n",
    "\n",
    "% Calculate standard errors\n",
    "ehat = Y - X*beta_IV;\n",
    "V_NHC_IV = var(ehat) * inv(Z_IV'*X)*Z_IV'*Z_IV*inv(Z_IV'*X);\n",
    "V_HC0_IV = inv(Z_IV'*X)*Z_IV' * diag(ehat.^2) * Z_IV*inv(Z_IV'*X);\n",
    "\n",
    "% 2SLS: l=3 instruments\n",
    "Pz = Z*inv(Z'*Z)*Z';\n",
    "beta_2SLS = (X'*Pz*X)\\(X'*Pz*Y) % = 2.0723, -0.9628\n",
    "\n",
    "% Calculate standard errors\n",
    "ehat = Y - X*beta_2SLS;\n",
    "V_NCH_2SLS = var(ehat) * inv(X'*Pz*X);\n",
    "V_HC0_2SLS = inv(X'*Pz*X)*X'*Pz * diag(ehat.^2) *Pz*X*inv(X'*Pz*X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting: we have a system of $L$ moment conditions\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t& \\mathbb E[g_1(\\omega_i, \\delta_0)] = 0 \\\\\n",
    "\t& \\vdots \\\\\n",
    "\t& \\mathbb E[g_L(\\omega_i, \\delta_0)] = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $L = \\dim (\\delta_0)$, no problem. If $L > \\dim (\\delta_0)$, there may be no solution to the system of equations. There are two possibilities.\n",
    "\n",
    "1. **First Solution**: add moment conditions until the system is identified\n",
    "\t$$\n",
    "\t  \\mathbb E[ a' g(\\omega_i, \\delta_0)] = 0\n",
    "\t$$\n",
    "  Solve $\\mathbb E[Ag(\\omega_i, \\delta)] = 0$ for $\\hat{\\delta}$. How to choose $A$? Such that it minimizes $Var(\\hat{\\delta})$.\t\n",
    "2. **Second Solution**: generalized method of moments (GMM)\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t  \\hat{\\delta} _ {GMM} &= \\arg \\min _ \\delta \\quad  \\Big| \\Big| \\mathbb E_n [ g(\\omega_i, \\delta) ] \\Big| \\Big| = \\\\\n",
    "\t  &= \\arg \\min _ \\delta \\quad n \\mathbb E_n[g(\\omega_i, \\delta)]' W \\mathbb E_n [g(\\omega_i, \\delta)]\n",
    "\t\\end{aligned}\n",
    "\t$$\n",
    "\t\n",
    "> The choice of $A$ and $W$ are closely related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-step GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $J(\\delta,W)$ is a quadratic form, a closed form solution exists:\n",
    "$$\n",
    "\t\\hat{\\delta}(W) = \\Big(\\mathbb E_n[z_i x_i'] W \\mathbb E_n[z_i x_i'] \\Big)^{-1}\\mathbb E_n[z_i x_i'] W \\mathbb E_n[z_i y_i]\n",
    "$$\n",
    "\n",
    "\n",
    "**Assumptions** for consistency of the GMM estimator given data $\\mathcal D = \\{y_i, x_i, z_i \\} _ {i=1}^n$:\n",
    "\n",
    "- **Linearity**: $y_i = x_i\\gamma_0 + \\varepsilon_i$\n",
    "- **IID**: $(y_i, x_i, z_i)$ iid\n",
    "- **Orthogonality**: $\\mathbb E [z_i(y_i - x_i\\gamma_0)] = \\mathbb E[z_i \\varepsilon_i] = 0$\n",
    "- **Rank identification**: $\\Sigma_{xz} = \\mathbb E[z_i x_i']$ has full rank \n",
    "\n",
    "**Theorem**:\n",
    "Under linearity, independence, orthogonality and rank conditions, if $\\hat{W} \\overset{p}{\\to} W$ positive definite, then \n",
    "$$\n",
    "\t\\hat{\\delta}(\\hat{W}) \\to \\delta(W)\n",
    "$$\n",
    "If in addition to the above assumption, $\\sqrt{n} \\mathbb E_n [g(\\omega_i, \\delta_0)] \\overset{d}{\\to} N(0,S)$ for a fixed positive definite $S$, then\n",
    "$$\n",
    "\t\\sqrt{n} (\\hat{\\delta} (\\hat{W}) - \\delta(W)) \\overset{d}{\\to} N(0,V)\n",
    "$$\n",
    "where $V = (\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1} \\Sigma _ {xz} W S W \\Sigma _ {xz}(\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$.\n",
    "\n",
    "Finally, if a consistent estimator $\\hat{S}$ of $S$ is available, then using sample analogues $\\hat{\\Sigma}_{xz}$ it follows that \n",
    "$$\n",
    "\t\\hat{V} \\overset{p}{\\to} V\n",
    "$$\n",
    "\n",
    "> If $W = S^{-1}$ then $V$ reduces to $V = (\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$. Moreover, $(\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$ is the smallest possible form of $V$, in a positive definite sense.\n",
    "\n",
    "Therefore, to have an efficient estimator, you want to construct $\\hat{W}$ such that $\\hat{W} \\overset{p}{\\to} S^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-step GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation steps:\n",
    "\n",
    "- Choose an arbitrary weighting matrix $\\hat{W}_{init}$ (usually the identity matrix $I_K$)\n",
    "- Estimate $\\hat{\\delta} _ {init}(\\hat{W} _ {init})$\n",
    "- Estimate $\\hat{S}$ (asymptotic variance of the moment condition) \n",
    "- Estimate $\\hat{\\delta}(\\hat{S}^{-1})$\n",
    "\n",
    "> On the procedure:\n",
    ">\n",
    "- This estimator achieves the semiparametric efficiency bound.\n",
    "- This strategy works only if $\\hat{S} \\overset{p}{\\to} S$ exists.\n",
    "- For iid cases: we can use $\\hat{\\delta} = \\mathbb E_n[(\\hat{\\varepsilon}_i z_i)(\\hat{\\varepsilon}_i z_i) ' ]$ where $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta}(\\hat{W} _ {init})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% GMM 1-step: inefficient weighting matrix\n",
    "W_1 = eye(l);\n",
    "\n",
    "% Objective function\n",
    "gmm_1 = @(b) ( Y - X*b )' * Z * W_1 *  Z' * ( Y - X*b );\n",
    "\n",
    "% Estimate GMM\n",
    "beta_gmm_1 = fminsearch(gmm_1, beta_OLS) % = 2.0763, -0.9548\n",
    "ehat = Y - X*beta_gmm_1;\n",
    "\n",
    "% Standard errors GMM\n",
    "S_hat = Z'*diag(ehat.^2)*Z;\n",
    "d_hat = -X'*Z;\n",
    "V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat');\n",
    "\n",
    "% GMM 2-step: efficient weighting matrix\n",
    "W_2 = inv(S_hat);\n",
    "gmm_2 = @(b) ( Y - X*b )' * Z * W_2 *  Z' * ( Y - X*b );\n",
    "beta_gmm_2 = fminsearch(gmm_2, beta_OLS) % = 2.0595, -0.9666\n",
    "\n",
    "% Standard errors GMM\n",
    "ehat = Y - X*beta_gmm_2;\n",
    "S_hat = Z'*diag(ehat.^2)*Z;\n",
    "d_hat = -X'*Z;\n",
    "V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Overidentifying Restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the equations are **exactly identified**, then it is possible to choose $\\delta$ so that all the elements of the sample moments $\\mathbb E_n[g(\\omega_i; \\delta)]$ are zero and thus that the distance\n",
    "$$\n",
    "J(\\delta, \\hat{W}) = n \\mathbb E_n[g(\\omega_i, \\delta)]' \\hat{W} \\mathbb E_n[g(\\omega_i, \\delta)]\n",
    "$$\n",
    "is zero. (The $\\delta$ that does it is the IV estimator.)\n",
    "\n",
    "If the equations are **overidentified**, i.e. $L$ (number of instruments) $> K$ (number of equations), then the distance cannot be zero exactly in general, but we would expect the minimized distance to be *close* to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose your model is overidentified ($L > K$) and you use the following naive testing procedure:\n",
    "\n",
    "1. Estimate $\\hat{\\delta}$ using a subset of dimension $K$ of instruments $\\{z_1 , .. , z_K\\}$ for $\\{x_1 , ... , x_K\\}$\n",
    "2. Set $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta} _ {\\text{GMM}}$\n",
    "3. Infer the size of the remaining $L-K$ moment conditions $\\mathbb E[z _{i, K+1} \\varepsilon_i], ..., \\mathbb E[z _{i, L} \\varepsilon_i]$ looking at their empirical counterparts $\\mathbb E_n[z _{i, K+1} \\hat{\\varepsilon}_i], ..., \\mathbb E_n[z _{i, L} \\hat{\\varepsilon}_i]$\n",
    "4. Reject exogeneity if the empirical expectations are high. How high? Calculate p-values. \n",
    "\n",
    "> **Example**\n",
    "> If you have two invalid instruments and you use one to test the validity of the other, it might happen by chance that you don't reject it.\n",
    ">\n",
    "- Model: $y_i = x_i + \\varepsilon_i$ and $x_i = \\frac{1}{2} z _{i1} -  \\frac{1}{2} z _{i2} + u_i$\n",
    "- Have $$\n",
    "  Cov (z _{i1}, z _{i2}, \\varepsilon_i, u_i) = \n",
    "  \\begin{bmatrix}\n",
    "\t\t1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0.5 & 1 \n",
    "\t\\end{bmatrix}\n",
    "\t$$\n",
    "- You want to test whether the second instrument is valid (is not since $\\mathbb E[z_2 \\varepsilon] \\neq 0$).\n",
    "\t\tYou use $z_1$ and estimate $\\hat{\\beta} \\to$ the estimator is consistent.\n",
    "- You obtain $\\mathbb E_n[z _{i2} \\hat{\\varepsilon}_i] \\simeq 0$ even if $z_2$ is invalid\n",
    "- Problem: you are using an invalid instrument in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansen's Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "We are interested in testing $H_0: \\mathbb E[z_i \\varepsilon_i] = 0$ against $H_1: \\mathbb E[z_i \\varepsilon_i] \\neq 0$. Suppose $\\hat{S} \\overset{p}{\\to} S$. Then \n",
    "$$\n",
    "\tJ(\\hat{\\delta}(\\hat{S}^{-1}) , \\hat{S}^{-1}) \\overset{d}{\\to} \\chi^2 _ {L-K}\n",
    "$$\n",
    "For $c$ satisfying $\\alpha = 1- G_{L - K} ( c )$, $\\Pr(J>c | H_0) \\to \\alpha$ so the test *reject $H_0$ if $J > c$* has asymptotic size $\\alpha$.\n",
    "\n",
    "> On Hansen's test:\n",
    ">\n",
    "- The degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions.\n",
    "- This is a specification test, testing whether all model assumptions are true jointly. Only when we are confident that about the other assumptions, can we interpret a large $J$ statistic as evidence for the endogeneity of some of the $L$ instruments included in $x$.\n",
    "- Unlike the tests we have encountered so far, the test is not consistent against some failures of the orthogonality conditions (that is, it is not consistent against some fixed elements of the alternative).\n",
    "- Several papers in the July 1996 issue of JBES report that the finite-sample null rejection probability of the test can far exceed the nominal significance level $\\alpha$. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: Conditional Homoskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main implication of conditional homoskedasticity is that efficient GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is $\\hat{S}^{-1} =  \\mathbb En [z_i z_i' \\varepsilon_i^2]^{-1}$. With conditional homoskedasticity,  the efficient weighting matrix is $\\mathbb E_n[z_iz_i']^{-1} \\sigma^{-2}$, or equivalently $\\mathbb E_n[z_iz_i']^{-1}$. Then, the GMM estimator becomes\n",
    "$$\n",
    "\t\\hat{\\delta}(\\hat{S}^{-1}) = \\Big(\\mathbb E_n[z_i x_i']' \\underbrace{\\mathbb E_n[z_iz_i']^{-1} \\mathbb E[z_i x_i']} _ {\\text{ols of } x_i \\text{ on }z_i} \\Big)^{-1}\\mathbb E_n[z_i x_i']' \\underbrace{\\mathbb E_n[z_iz_i']^{-1} \\mathbb E[z_i y_i']} _ {\\text{ols of } y_i \\text{ on }z_i}= \\hat{\\delta} _ {2SLS}\n",
    "$$\n",
    "\n",
    "**Proof**:\n",
    "Consider the matrix notation.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\hat{\\delta} \\left( \\frac{Z'Z}{n}\\right) &= \\left( \\frac{X'Z}{n} \\left( \\frac{Z'Z}{n}\\right)^{-1} \\frac{Z'X}{n} \\right)^{-1} \\frac{X'Z}{n} \\left( \\frac{Z'Z}{n}\\right)^{-1} \\frac{Z'Y}{n} = \\\\\n",
    "\t&= \\left( X'Z(Z'Z)^{-1} Z'X \\right)^{-1} X'Z(Z'Z)^{-1} Z'Y = \\\\\n",
    "\t&= \\left(X'P_ZX\\right)^{-1} X'P_ZY = \\\\\n",
    "\t&= \\left(X'P_ZP_ZX\\right)^{-1} X'P_ZY = \\\\\n",
    "\t&= \\left(\\hat{X}'_Z \\hat{X}_Z\\right)^{-1} \\hat{X}'_ZY = \\\\\n",
    "\t&= \\hat{\\delta} _ {2SLS}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tag*{$\\blacksquare$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small-Sample Properties of 2SLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "When the number of instruments is equal to the sample size ($L = n$), then $\\hat{\\delta} _ {2SLS} = \\hat{\\delta} _ {OLS}$\n",
    "\n",
    "**Proof**:\n",
    "We have a perfect prediction problem. The first stage estimated coefficient $\\hat{\\gamma}$ is such that it solves the normal equations: $\\hat{\\gamma} = z_i^{-1} x_i$. Then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\hat{\\delta} _ {2SLS} &= \\mathbb E_n[\\hat{x}_i x'_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] = \\\\\n",
    "\t&= \\mathbb E_n[z_i z_i^{-1} x_i x'_i]^{-1} \\mathbb E_n[z_i z_i^{-1} x_i y_i] = \\\\\n",
    "\t&= \\mathbb E_n[x_i x'_i]^{-1} \\mathbb E_n[x_i y_i] = \\\\\n",
    "\t&= \\hat{\\delta} _ {OLS}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "> You have this overfitting problem in general when the number of instruments is large relative to the sample size. This problem arises even if the instruments are valid. \n",
    "\n",
    "\n",
    "> **Example** from Angrist (1992)\n",
    ">\n",
    "- They regress wages on years of schooling.\n",
    "- Problem: endogeneity: both variables are correlated with skills which are unobserved.\n",
    "- Solution: instrument years of schooling with the quarter of birth. Idea: if born in the first three quarters, can attend school from the year of your sixth birthday. Otherwise, you have to wait one more year. \n",
    "- Problem: quarters of birth are three dummies. In order to ``improve the first stage fit\" they interact them with year of birth (180 effective instruments) and also with the state (1527 effective instruments). This mechanically increases the $R^2$ but also increases the bias of the 2SLS estimator.\n",
    "- Solutions: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso (Belloni et al., 2012).\n",
    "\n",
    "![RJIVE](figures/Fig_441.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many Instrument Robust Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why having too many instruments is problematic? As the number of instruments increases, the estimated coefficient gets closer to OLS which is biased. As seen in the theorem above, for $L=n$, the two estimators coincide. \n",
    "\n",
    "![IV to OLS](figures/Fig_451.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of $(y_i, x_i)$. The estimator is known as **limited information maximum likelihood**, or **LIML**.\n",
    "This estimator is called ``limited information\" because it is based on the structural equation for $(y_i, x_i)$ combined with the reduced form equation for $x_i$. If maximum likelihood is derived based on a structural equation for $x_i$ as well, then this leads to what is known as **full information maximum likelihood (FIML)**. The advantage of the LIML approach relative to FIML is that the former does not require a structural model for $x_i$, and thus allows the researcher to focus on the structural equation of interest - that for $y_i$. \n",
    "\n",
    "\n",
    "The **k-class** estimators have the form\n",
    "$$\n",
    "\t\\hat{\\delta}(\\alpha) = (X' P_Z X - \\alpha X' X)^{-1} (X' P_Z Y - \\alpha X' Y) \n",
    "$$\n",
    "\n",
    "The limited information maximum likelihood estimator **LIML** is the k-class estimator $\\hat{\\delta}(\\alpha)$ where\n",
    "$$\n",
    "\t\\alpha = \\lambda_{min} \\Big( ([X' , Y]^{-1} [X' , Y])^{-1} [X' , Y]^{-1} P_Z [X' , Y] \\Big) \n",
    "$$\n",
    "\n",
    "If $\\alpha = 0$ then $\\hat{\\delta} _ {\\text{LIML}} = \\hat{\\delta} _ {\\text{2SLS}}$ while for $\\alpha \\to \\infty$, $\\hat{\\delta} _ {\\text{LIML}} \\to \\hat{\\delta} _ {\\text{OLS}}$.\n",
    "\n",
    ">\n",
    "Comments on LIML:\n",
    ">\n",
    "- The particular choice of $\\alpha$ gives a many instruments robust estimate\n",
    "- The LIML estimator has no finite sample moments. $\\mathbb E[\\delta(\\alpha_{LIML})]$ does not exist in general\n",
    "- In simulation studies performs well\n",
    "- Has good asymptotic properties \n",
    "\n",
    "\n",
    "Asymptotically the LIML estimator has the same distribution as 2SLS. However, they can have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has superior finite sample performance to 2SLS when there are many instruments or the reduced form is weak. However, on the other hand there is worry that since the LIML estimator is derived under normality it may not be robust in non-normal settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Jacknife IV** procedure is the following\n",
    "\n",
    "- Regress $\\{ x_j \\} _ {j \\neq i}$ on $\\{ z_j \\} _ {j \\neq i}$ and estimate $\\pi_{-i}$ (leave the $i^{th}$ observation out).\n",
    "- Form $\\hat{x}_i = \\hat{\\pi} _ {-i} z_i$.\n",
    "- Run IV using $\\hat{x}_i$ as instruments.\n",
    "$$\n",
    "\t\\hat{\\delta} _ {JIVE} = \\mathbb E_n[\\hat{x}_i x_i']^{-1} \\mathbb E_n[\\hat{x}_i y_i']\n",
    "$$\n",
    "\n",
    "> Comments on JIVE:\n",
    ">\n",
    "- Prevents overfitting.\n",
    "- With many instruments you get bad out of sample prediction which implies low correlation between $\\hat{x}_i$ and $x_i$: $\\mathbb E_n[\\hat{x}_i x_i'] \\simeq 0$.\n",
    "- Use lasso/ridge regression in the first stage in case of too many instruments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hausman Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider testing the validity of OLS. OLS is generally preferred to IV in terms of precision. Many researchers only doubt the (joint) validity of the regressor $z_i$ instead of being certain that it is invalid (in the sense of not being predetermined). So then they wish to choose between OLS and 2SLS, assuming that they have an instrument vector $x_i$ whose validity is not in question. Further, assume for simplicity that $L = K$ so that the efficient GMM estimator is the IV estimator.\n",
    "\n",
    "\n",
    "The **Hausman test statistic**\n",
    "$$\n",
    "\tH \\equiv n (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})' [\\hat{Avar} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})]^{-1} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})\n",
    "$$\n",
    "is asymptotically distributed as a $\\chi^2_{L-s}$ under the null where $s = \\# z_i \\cup x_i$: the number of regressors that are retained as instruments in $x_i$.\n",
    "\n",
    ">\n",
    "In general, the idea of the Hausman test is the following. If you have two estimators, one which is efficient under $H_0$ but inconsistent under $H_1$ (in this case, OLS), and another which is consistent under $H_1$ (in this case, IV), then construct a test as a quadratic form in the differences of the estimators. Another classic example arises in panel data with the hypothesis $H_0$ of unconditional strict exogeneity. In that case, under $H_0$ Random Effects estimators are efficient but under $H_1$ they are inconsistent. Fixed Effects estimators instead are consistent under $H_1$.\n",
    ">\n",
    "The Hausman test statistic can be used as a pretest procedure: select either OLS or IV according to the outcome of the test. Although widely used, this pretest procedure is not advisable. When the null is false, it is still possible that the test *accepts* the null (committing a Type 2 error). In particular, this can happen with a high probability when the sample size is *small* and/or when the regressor $z_i$ is *almost valid*. In such an instance, estimation and also inference will be based on incorrect methods. Therefore, the overall properties of the Hausman pretest procedure are undesirable.\n",
    ">\n",
    "The Hausman test is an example of a specification test. There are many other specification tests. One could for example test for conditional homoskedasticity. Unlike for the OLS case, there does not exist a convenient test for conditional homoskedasticity for the GMM case. A test statistic that is asymptotically chi-squared under the null is available but is extremely cumbersome; see White (1982, note 2). If in doubt, it is better to use the more generally valid inference methods that allow for conditional heteroskedasticity. Similarly, there does not exist a convenient test for serial correlation for the GMM case. If in doubt, it is better to use the more generally valid inference methods that allow for serial correlation; for example, when data are collected over time (that is, time-series data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Belloni, A., Chen, H., Chernozhukov, V., & Hansen, C. B. (2012). *Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain*. Econometrica, 80(6), 2369–2429.\n",
    "- Hansen (2019). \"*Econometrics*\". Chapters 12 and 13.\n",
    "- Hayiashi (2000). \"*Econometrics*\".\n",
    "- Kozbur (2019). PhD Econometrics - Lecture Notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
