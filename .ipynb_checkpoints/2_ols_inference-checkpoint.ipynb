{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [Asymptotic Theory of the OLS Estimator](#Asymptotic-Theory-of-the-OLS-Estimator)\n",
    " - [Inference](#Inference)\n",
    " - [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import julia packages \n",
    "using Random, LinearAlgebra, Statistics, StatsBase, Plots, StatsFuns\n",
    "default(size=(2000,1600), legend=false, thickness_scaling=3, linewidth=3)\n",
    "\n",
    "# Set seed\n",
    "Random.seed!(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap from last time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of observations\n",
    "n = 100;\n",
    "\n",
    "# Set the dimension of X\n",
    "k = 2;\n",
    "\n",
    "# Draw a sample of explanatory variables\n",
    "X = rand(n, k);\n",
    "\n",
    "# Draw the error term\n",
    "sigma = 1;\n",
    "e = randn(n)*sqrt(sigma);\n",
    "\n",
    "# Set the parameters\n",
    "b = [2, -1];\n",
    "\n",
    "# Calculate the dependent variable\n",
    "y = X*b + e;\n",
    "\n",
    "# Equivalent but faster formulation\n",
    "b_hat = (X'*X)\\(X'*y)\n",
    "\n",
    "# Residuals\n",
    "e_hat = y - X*b_hat;\n",
    "\n",
    "# Leverage\n",
    "h = Diagonal(X*inv(X'*X)*X');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Theory of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\\mathbb E[x_i x_i'] = Q$ positive definite, $\\mathbb E[x_i x_i'] < \\infty$ and $\\mathbb E [y_i^2] < \\infty$, then $\\hat \\beta _ {OLS}$ is a **consistent** estimator of $\\beta_0$, i.e. $\\hat \\beta = \\mathbb E_n [x_i x_i'] \\mathbb E_n [x_i y_i]\\overset{p}{\\to} \\beta_0$.\n",
    "\n",
    "**Proof**:  \n",
    "We consider 4 steps:\n",
    "\n",
    "1. $\\mathbb E_n [x_i x_i'] \\xrightarrow{p} \\mathbb E [x_i x_i']$ by WLLN since $x_i x_i'$ iid and $\\mathbb E[x_i x_i'] < \\infty$.\n",
    "2. $\\mathbb E_n [x_i y_i] \\xrightarrow{p} \\mathbb E [x_i y_i]$ by WLLN, due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of $x_i$ and $y_i$\n",
    "  $$\n",
    "\t  \\mathbb E \\left[ x_i y_i \\right]  \\leq \\sqrt{ \\mathbb E[x_i^2] \\mathbb E[y_i^2]} < \\infty\n",
    "  $$\n",
    "3. $\\mathbb E_n [x_i x_i']^{-1} \\xrightarrow{p} \\mathbb E [x_i x_i']^{-1}$ by CMT.\n",
    "4. $\\mathbb E_n [x_i x_i']^{-1} \\mathbb E_n [x_i y_i] \\xrightarrow{p} \\mathbb E [x_i x_i']^{-1} \\mathbb E [x_i y_i] = \\beta$ by CMT.\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "Now we are going to investigate the variance of $\\hat \\beta _ {OLS}$ progressively relaxing the underlying assumptions.\n",
    "\n",
    "- Gaussian error term.\n",
    "- Homoskedastic error term.\n",
    "- Heteroskedastic error term.\n",
    "- Heteroskedastic and autocorrelated error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Error Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**: \n",
    "Under the GM assumption (1)-(5), $\\hat \\beta - \\beta |X \\sim N(0, \\sigma^2 (X'X)^{-1})$\n",
    "\n",
    "**Proof**:  \n",
    "We follow 2 steps:\n",
    "\n",
    "1. We can rewrite $\\hat \\beta$ as\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t\t\\hat \\beta & = (X'X)^{-1} X'y = (X'X)^{-1} X'(X\\beta + \\varepsilon) \\\\\n",
    "\t\t&= \\beta + (X'X)^{-1} X' \\varepsilon = \\\\\n",
    "\t\t&= \\beta + \\mathbb E_n [x_i x_i']^{-1} \\mathbb E_n [x_i \\varepsilon_i]\n",
    "\t\\end{aligned}\n",
    "\t$$\n",
    "2. Therefore: $\\hat \\beta-\\beta  = \\mathbb E_n [x_i x_i']^{-1} \\mathbb E_n [x_i \\varepsilon_i]$.\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t\t\\hat \\beta-\\beta |X & \\sim (X'X)^{-1} X' N(0, \\sigma^2 I_n) = \\\\\n",
    "\t\t&= N(0, \\sigma^2 (X'X)^{-1} X'X (X'X)^{-1}) = \\\\\n",
    "\t\t&= N(0, \\sigma^2 (X'X)^{-1})\n",
    "\t\\end{aligned}\n",
    "\t$$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "> Does it make sense to assume that $\\varepsilon$ is gaussian? Not much. But does it make sense to assume that $\\hat \\beta$ is gaussian? Yes, because it's an average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoskedastic Error Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**: \n",
    "\tUnder the assumptions of the previous theorem, plus $\\mathbb E[x^4] < \\infty$, the OLS estimate has an asymptotic normal distribution: $\\hat \\beta|X \\overset{d}{\\to} N(\\beta, \\sigma^2 (X'X)^{-1})$.\n",
    "\n",
    "**Proof**: \n",
    "\t$$\n",
    "\t\\sqrt{n} (\\hat \\beta - \\beta ) = \\underbrace{\\mathbb E_n [x_i x_i']^{-1}} _ {\\xrightarrow{p} Q^{-1} }   \\underbrace{\\sqrt{n} \\mathbb E_n [x_i \\varepsilon_i ]} _ {\\xrightarrow{d} N(0, \\Omega)} \\rightarrow N(0, \\Sigma )\n",
    "\t$$\n",
    "\twhere in general $\\Omega = Var (x_i \\varepsilon_i) = \\mathbb E [(x_i \\varepsilon_i)^2]$ and $\\Sigma = Q^{-1} \\Omega Q^{-1}$. \\\\\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "> Given that $Q = \\mathbb E [x_i x_i']$ is unobserved, we estimate it with $\\hat{Q} = \\mathbb E_n [x_i x_i']$. Since we have assumed homoskedastic error term, we have $\\Omega =  \\sigma^2 (X'X)^{-1}$. Since we do not observe $\\sigma^2$ we estimate it as $\\hat{\\sigma}^2 = \\mathbb E_n[\\hat{\\varepsilon}_i^2]$.\n",
    "\n",
    "The terms $x_i \\varepsilon_i$ are called **scores** and we can already see their central importance for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroskedastic Error Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption**:\n",
    "$\\mathbb E [\\varepsilon_i x_i \\varepsilon_j' x_j'] = 0$, for all $j \\ne i$ and $\\mathbb E [\\varepsilon_i^4] \\leq \\infty$, $\\mathbb E [|| x_i||^4] \\leq C < \\infty$ a.s.\n",
    "\n",
    "**Theorem**: \n",
    "Under GM assumptions (1)-(4) plus heteroskedastic error term, the following estimators are consistent, i.e. $\\hat{\\Sigma}\\xrightarrow{p} \\Sigma$.\n",
    "\n",
    "> Note that we are only specifying $\\Omega$ of the $\\Sigma = Q^{-1} \\Omega Q^{-1}$ matrix.\t\t\n",
    "\n",
    "- **HC0**: use the observed residual $\\hat{\\varepsilon}_i$\n",
    "  $$\n",
    "\t\t\\Omega _ {HC0} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2]\n",
    "\t$$\n",
    "\tWhen $k$ is too big relative to $n$ -- i.e., $k/n \\rightarrow c >0$ --  $\\hat{\\varepsilon}_i^2$ are too small ($\\Omega _ {HC0}$ biased towards zero). $\\Omega _ {HC1}$, $\\Omega _ {HC2}$ and $\\Omega _ {HC3}$ try to correct this small sample bias. \\\\\n",
    "- **HC1**: degree of freedom correction (default `robust` in Stata)\n",
    "\t$$\n",
    "\t\t\\Omega _ {HC1} = \\frac{1}{n - k }\\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2] \n",
    "\t$$\n",
    "- **HC2**: use standardized residuals\n",
    "\t$$\n",
    "\t\t\\Omega _ {HC2} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2 (1-h _ {ii})^{-1}]\n",
    "\t$$\n",
    "\twhere $h _ {ii} = [X(X'X)^{-1} X'] _ {ii}$ is the **leverage** of the $i^{th}$ observation. A large $h _ {ii}$  means that observation $i$ is unusual in the sense that the regressor $x_i$ is far from its sample mean.\n",
    "- **HC3**: use prediction error, equivalent to Jack-knife estimator, i.e., $\\mathbb E_n [x_i x_i' \\hat{\\varepsilon} _ {(-i)}^2]$\n",
    "\t$$\n",
    "\t\t\\Omega _ {HC3} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2 (1-h _ {ii})^{-2}]\n",
    "\t$$\n",
    "\tThis estimator does not overfit when $k$ is relatively big with respect to $n$. Idea: you exclude the corresponding observation when estimating a particular $\\varepsilon_i$: $\\hat{\\varepsilon}_i = y_i - x_i' \\hat \\beta _ {-i}$.\n",
    "\n",
    "**Theorem**:\n",
    "Under regularity conditions HC0 is consistent, i.e. $\\hat{\\Sigma} _ {HC0} \\overset{p}{\\to} \\Sigma$.\n",
    "$$\n",
    "\t\\hat{\\Sigma} = \\hat{Q}^{-1} \\hat{\\Omega} \\hat{Q}^{-1} \\xrightarrow{p} \\Sigma \\qquad  \\text{ with } \\hat{\\Omega} = \\mathbb E_n [x_i x_i' \t\\hat{\\varepsilon}_i^2] \\quad \\text{ and } \\hat{Q} = \\mathbb E_n [x_i x_i']^{-1} \n",
    "$$\n",
    "\n",
    "> Why is the proof relevant? You cannot directly apply the WLLN to $\\hat \\Sigma$.\n",
    "\n",
    "**Proof**:  \n",
    "For the case $\\mathrm{dim}(x_i) =1$.\n",
    "\n",
    "1. $\\hat{Q}^{-1} \\xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid, $\\mathbb E[x_i^4] < \\infty$\n",
    "2. $\\bar{\\Omega} = \\mathbb E_n [\\varepsilon_i^2 x_i x_i'] \\xrightarrow{p} \\Omega$ by WLLN since $\\mathbb E_n [\\varepsilon_i^4] < c$ and $x_i$ bounded. \n",
    "3. By the triangle inequality,\n",
    "\t$$\n",
    "\t\t| \\hat{\\Omega} - \\hat{\\Omega}| \\leq \\underbrace{|\\Omega - \\bar{\\Omega}|} _ {\\overset{p}{\\to} 0} + \\underbrace{|\\bar{\\Omega} - \\hat{\\Omega}|} _ {\\text{WTS:} \\overset{p}{\\to} 0} \n",
    "\t$$\n",
    "4. We want to show $|\\bar{\\Omega} - \\hat{\\Omega}| \\overset{p}{\\to} 0$\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t\t|\\bar{\\Omega} - \\hat{\\Omega}| &= \\mathbb E_n [\\varepsilon_i^2 x_i^2] - \\mathbb E_n [\\hat{\\varepsilon}_i^2 x_i^2]  = \\\\\n",
    "\t\t&= \\mathbb E_n [\\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right) x_i^2] \\leq \\\\\n",
    "\t\t& \\leq \\mathbb E_n \\left[ \\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2\\right]^{\\frac{1}{2}} \\mathbb E_n [x_i^4]^{\\frac{1}{2}}\n",
    "\t\\end{aligned}\n",
    "\t$$\n",
    "\twhere $\\mathbb E_n [x_i^4]^{\\frac{1}{2}} \\xrightarrow{p}  \\mathbb E [x_i^4]^{\\frac{1}{2}}$ by $x_i$ bounded, iid and CMT.\n",
    "5. We want to show that $\\mathbb E_n \\left[ \\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2\\right] \\leq \\eta$ with $\\eta \\rightarrow 0$.\n",
    "\tLet $L = \\max_i |\\hat{\\varepsilon}_i - \\varepsilon_i|$ (RV depending on $n$), with $L \\xrightarrow{p} 0$ since\n",
    "\t$$\n",
    "\t\t|\\hat{\\varepsilon}_i - \\varepsilon_i| = |x_i \\hat \\beta - x_i \\beta| \\leq |x_i||\\hat \\beta - \\beta|\\xrightarrow{p} c \\cdot 0 \n",
    "\t$$\n",
    "\tWe can depompose\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t\t\\left(\\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2 & = \\left(\\varepsilon_i - \\hat{\\varepsilon}_i \\right)^2 \\left(\\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 \\leq \\\\\\\n",
    "\t\t& \\leq \\left(\\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 L^2 = \\\\\n",
    "\t\t &= \\left(2\\varepsilon_i - \\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 L^2\\leq  \\\\\n",
    "\t\t & \\leq  \\left( 2(2\\varepsilon_i)^2 + 2(\\hat{\\varepsilon}_i - \\varepsilon_i)^2 \\right)^2 L^2 \\leq \\\\\n",
    "\t\t & \\leq (8 \\varepsilon_i^2 + 2 L^2) L^2\n",
    "\t \\end{aligned}\n",
    "\t$$\n",
    "\tHence\n",
    "  $$\n",
    "\t \\mathbb E \\left[ \\left(\\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2 \\right] \\leq  L^2 \\left( 8 \\mathbb E_n [ \\varepsilon_i^2] + 2 \\mathbb E_n [L^2] \\right)  \\xrightarrow{p}0\n",
    "\t$$\n",
    "$$\\tag*{$\\blacksquare$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroskedastic and Autocorrelated Error Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption**:\n",
    "There esists a $\\bar{d}$ such that:\n",
    "\t\n",
    "- $\\mathbb E[\\varepsilon_i x_i \\varepsilon' _ {i-d} x' _ {i-d}] \\neq 0 \\quad$ for $d \\leq \\bar{d}$\n",
    "- $\\mathbb E[\\varepsilon_i x_i \\varepsilon' _ {i-d} x' _ {i-d}] = 0 \\quad$ for $d > \\bar{d}$\n",
    "\n",
    "> Intuition: observations far enough from each other are not correlated.\\\\\n",
    "\n",
    "We can express the variance of the score as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\Omega_n &= Var(\\sqrt{n} \\mathbb E_n[x_i \\varepsilon_i]) = \\\\\n",
    "\t&= \\mathbb E \\left[ \\left( \\frac{1}{n} \\sum _ {i=1}^n x_i \\varepsilon_i \\right) \\left( \\frac{1}{n} \\sum _ {j=1}^n x_j \\varepsilon_j \\right) \\right] = \\\\\n",
    "\t&= \\frac{1}{n} \\sum _ {i=1}^n \\sum _ {j=1}^n \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j'] = \\\\\n",
    "\t&= \\frac{1}{n} \\sum _ {i=1}^n \\sum _ {j : |i-j|\\leq \\bar{d}} \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j'] = \\\\\n",
    "\t&= \\frac{1}{n} \\sum _ {d=0}^{\\bar{d}} \\sum _ {i = d}^{n} \\mathbb E[x_i \\varepsilon_i x _ {i-d}' \\varepsilon _ {i-d}'] \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We estimate $\\Omega_n$ by \n",
    "$$\n",
    "\t\\hat{\\Omega}_n = \\frac{1}{n} \\sum _ {d=0}^{\\bar{d}} \\sum _ {i = d}^{n} x_i \\hat{\\varepsilon}_i x _ {i-d}' \\hat{\\varepsilon} _ {i-d}'\n",
    "$$\n",
    "\n",
    "**Theorem**: \n",
    "If $\\bar{d}$ is a fixed integer, then\n",
    "$$\n",
    "\t\\hat{\\Omega}_n - \\Omega_n \\overset{p}{\\to} 0\n",
    "$$\n",
    "\n",
    "> What if $\\bar{d}$ does not exist (all $x_i, x_j$ are correlated)?\n",
    "$$\n",
    "\t\\hat{\\Omega}_n = \\frac{1}{n} \\sum _ {d=0}^{n} \\sum _ {i = d}^{n} x_i \\hat{\\varepsilon}_i x _ {i-d}' \\hat{\\varepsilon} _ {i-d}' = n \\mathbb E_n[x_i \\hat{\\varepsilon}_i]^2 = 0\n",
    "$$\n",
    "By the orthogonality property of the OLS residual.\n",
    "\n",
    "\n",
    "**HAC with Uniform Kernel**\n",
    "$$\n",
    "\t\\hat{\\Omega}_h = \\frac{1}{n} \\sum _ {i,j} x_i \\hat{\\varepsilon}_i x_j' \\hat{\\varepsilon}_j' \\mathbb{I} \\\\{ |i-j| \\leq h\\\\}  \n",
    "$$\n",
    "where $h$ is the **bandwidth** of the kernel. The bandwidth is chosen such that $\\mathbb E[x_i \\varepsilon_i  x _ {i-d}' \\varepsilon _ {i-d}' ]$ is small for $d > h$. How small? Small enough for the estimates to be consistent.\n",
    "\n",
    "**HAC with General Kernel**\n",
    "$$\n",
    "\\hat{\\Omega}^{HAC} _ {k,h} = \\frac{1}{n} \\sum _ {i,j} x_i \\hat{\\varepsilon}_i x_j' \\hat{\\varepsilon}_j' k \\left( \\frac{|i-j|}{n} \\right) \\\\\n",
    "$$\n",
    "\n",
    "**Theorem**:\n",
    "If the joint distribution is stationary and $\\alpha$-mixing with $\\sum _ {k=1}^\\infty k^2 \\alpha(k) < \\infty$ and\n",
    "\t\n",
    "- $\\mathbb E[ | x _ {ij} \\varepsilon_i |^\\nu ] < \\infty$ $\\forall \\nu$\n",
    "- $\\hat{\\varepsilon}_i = y_i - x_i' \\hat \\beta$ for some $\\hat \\beta \\overset{p}{\\to} \\beta_0$\n",
    "- $k$ smooth, symmetric, $k(0) \\to \\infty$ as $z \\to \\infty$, $\\int k^2 < \\infty$\n",
    "- $\\frac{h}{n} \\to 0$\n",
    "- $h \\to \\infty$\n",
    "\n",
    "Then the HAC estimator is **consistent**. \n",
    "$$\n",
    "\t\\hat{\\Omega}^{HAC} _ {k,h} - \\Omega_n \\overset{p}{\\to} 0\n",
    "$$\n",
    "\n",
    "We want to choose $h$ small relative to $n$ in order to avoid estimation problems. But we also want to choose $h$ large so that the remainder is small:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Omega_n &= Var(\\sqrt{n} \\mathbb E_n[x_i \\varepsilon_i]) = \\\\\n",
    "  &= \\underbrace{\\frac{1}{n} \\sum _ {i,j : |i-j|\\leq h} \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j']} _ {\\Omega^h_n} + \\underbrace{\\frac{1}{n} \\sum _ {i,j : |i-j|> h} \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j']} _ {\\text{remainder: } R_n} = \\\\\n",
    "  &= \\Omega_n^h + R_n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In particular, HAC theory requires:\n",
    "$$\n",
    "\t\\hat{\\Omega}^{HAC} \\overset{p}{\\to} \\Omega \\quad \\text{ if } \\quad \n",
    "\t\\begin{cases}\n",
    "\t& \\frac{h}{n} \\to 0 \\\\\n",
    "\t& h \\to \\infty\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "But in practice, long-run estimation implies $\\frac{h}{n} \\simeq 0$ which is not ``safe\" in the sense that it does not imply $R_n \\simeq 0$.\n",
    "On the other hand, if $h \\simeq n$, $\\hat{\\Omega}^{HAC}$ does not converge in probability because it's too noisy. \n",
    "\n",
    "**Example**:\n",
    "How to choose $h$? Look at the score autocorrelation function (ACF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelated process\n",
    "T = 1000\n",
    "v = rand(1000);\n",
    "for t=11:T\n",
    "    for p=1:10\n",
    "        v[t] += v[t-p]*0.3;\n",
    "    end\n",
    "end\n",
    "\n",
    "# Autocorrelation function\n",
    "A = autocor(v);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500\" height=\"400\" viewBox=\"0 0 2000 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip410\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip410)\" d=\"\n",
       "M0 1600 L2000 1600 L2000 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip411\">\n",
       "    <rect x=\"400\" y=\"0\" width=\"1401\" height=\"1401\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip410)\" d=\"\n",
       "M445.066 1271.33 L1964.57 1271.33 L1964.57 35.4331 L445.066 35.4331  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip412\">\n",
       "    <rect x=\"445\" y=\"35\" width=\"1521\" height=\"1237\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  679.203,1271.33 679.203,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  918.118,1271.33 918.118,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1157.03,1271.33 1157.03,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1395.95,1271.33 1395.95,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1634.86,1271.33 1634.86,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1873.78,1271.33 1873.78,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,1271.33 1964.57,1271.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  679.203,1271.33 679.203,1256.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  918.118,1271.33 918.118,1256.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  1157.03,1271.33 1157.03,1256.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  1395.95,1271.33 1395.95,1256.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  1634.86,1271.33 1634.86,1256.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  1873.78,1271.33 1873.78,1256.5 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip410)\" d=\"M 0 0 M671.912 1292.51 L685.679 1292.51 L685.679 1295.46 L675.123 1295.46 L675.123 1301.82 Q675.887 1301.56 676.651 1301.44 Q677.415 1301.3 678.179 1301.3 Q682.519 1301.3 685.054 1303.68 Q687.589 1306.05 687.589 1310.12 Q687.589 1314.3 684.984 1316.63 Q682.38 1318.94 677.641 1318.94 Q676.009 1318.94 674.307 1318.66 Q672.623 1318.38 670.818 1317.83 L670.818 1314.3 Q672.38 1315.15 674.047 1315.57 Q675.714 1315.98 677.571 1315.98 Q680.575 1315.98 682.328 1314.41 Q684.082 1312.83 684.082 1310.12 Q684.082 1307.41 682.328 1305.83 Q680.575 1304.25 677.571 1304.25 Q676.165 1304.25 674.759 1304.56 Q673.37 1304.87 671.912 1305.53 L671.912 1292.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M900.775 1315.48 L906.504 1315.48 L906.504 1295.71 L900.271 1296.96 L900.271 1293.76 L906.469 1292.51 L909.976 1292.51 L909.976 1315.48 L915.705 1315.48 L915.705 1318.43 L900.775 1318.43 L900.775 1315.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M927.007 1294.82 Q924.299 1294.82 922.927 1297.5 Q921.573 1300.15 921.573 1305.5 Q921.573 1310.83 922.927 1313.5 Q924.299 1316.16 927.007 1316.16 Q929.733 1316.16 931.087 1313.5 Q932.459 1310.83 932.459 1305.5 Q932.459 1300.15 931.087 1297.5 Q929.733 1294.82 927.007 1294.82 M927.007 1292.04 Q931.365 1292.04 933.657 1295.5 Q935.966 1298.94 935.966 1305.5 Q935.966 1312.04 933.657 1315.5 Q931.365 1318.94 927.007 1318.94 Q922.65 1318.94 920.341 1315.5 Q918.049 1312.04 918.049 1305.5 Q918.049 1298.94 920.341 1295.5 Q922.65 1292.04 927.007 1292.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1140.06 1315.48 L1145.79 1315.48 L1145.79 1295.71 L1139.56 1296.96 L1139.56 1293.76 L1145.76 1292.51 L1149.26 1292.51 L1149.26 1315.48 L1154.99 1315.48 L1154.99 1318.43 L1140.06 1318.43 L1140.06 1315.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1158.83 1292.51 L1172.6 1292.51 L1172.6 1295.46 L1162.04 1295.46 L1162.04 1301.82 Q1162.81 1301.56 1163.57 1301.44 Q1164.33 1301.3 1165.1 1301.3 Q1169.44 1301.3 1171.97 1303.68 Q1174.51 1306.05 1174.51 1310.12 Q1174.51 1314.3 1171.9 1316.63 Q1169.3 1318.94 1164.56 1318.94 Q1162.93 1318.94 1161.23 1318.66 Q1159.54 1318.38 1157.74 1317.83 L1157.74 1314.3 Q1159.3 1315.15 1160.97 1315.57 Q1162.63 1315.98 1164.49 1315.98 Q1167.49 1315.98 1169.25 1314.41 Q1171 1312.83 1171 1310.12 Q1171 1307.41 1169.25 1305.83 Q1167.49 1304.25 1164.49 1304.25 Q1163.08 1304.25 1161.68 1304.56 Q1160.29 1304.87 1158.83 1305.53 L1158.83 1292.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1381.81 1315.48 L1394.05 1315.48 L1394.05 1318.43 L1377.59 1318.43 L1377.59 1315.48 Q1379.59 1313.42 1383.02 1309.94 Q1386.48 1306.45 1387.36 1305.45 Q1389.05 1303.55 1389.71 1302.25 Q1390.38 1300.93 1390.38 1299.67 Q1390.38 1297.6 1388.93 1296.3 Q1387.49 1295 1385.16 1295 Q1383.51 1295 1381.67 1295.57 Q1379.85 1296.14 1377.76 1297.3 L1377.76 1293.76 Q1379.88 1292.91 1381.72 1292.48 Q1383.56 1292.04 1385.09 1292.04 Q1389.12 1292.04 1391.51 1294.06 Q1393.91 1296.07 1393.91 1299.44 Q1393.91 1301.04 1393.3 1302.48 Q1392.71 1303.9 1391.13 1305.85 Q1390.7 1306.35 1388.37 1308.76 Q1386.04 1311.16 1381.81 1315.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1405.35 1294.82 Q1402.64 1294.82 1401.27 1297.5 Q1399.92 1300.15 1399.92 1305.5 Q1399.92 1310.83 1401.27 1313.5 Q1402.64 1316.16 1405.35 1316.16 Q1408.08 1316.16 1409.43 1313.5 Q1410.8 1310.83 1410.8 1305.5 Q1410.8 1300.15 1409.43 1297.5 Q1408.08 1294.82 1405.35 1294.82 M1405.35 1292.04 Q1409.71 1292.04 1412 1295.5 Q1414.31 1298.94 1414.31 1305.5 Q1414.31 1312.04 1412 1315.5 Q1409.71 1318.94 1405.35 1318.94 Q1400.99 1318.94 1398.68 1315.5 Q1396.39 1312.04 1396.39 1305.5 Q1396.39 1298.94 1398.68 1295.5 Q1400.99 1292.04 1405.35 1292.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1621.1 1315.48 L1633.34 1315.48 L1633.34 1318.43 L1616.88 1318.43 L1616.88 1315.48 Q1618.87 1313.42 1622.31 1309.94 Q1625.77 1306.45 1626.65 1305.45 Q1628.34 1303.55 1629 1302.25 Q1629.67 1300.93 1629.67 1299.67 Q1629.67 1297.6 1628.21 1296.3 Q1626.77 1295 1624.45 1295 Q1622.8 1295 1620.96 1295.57 Q1619.13 1296.14 1617.05 1297.3 L1617.05 1293.76 Q1619.17 1292.91 1621.01 1292.48 Q1622.85 1292.04 1624.38 1292.04 Q1628.41 1292.04 1630.8 1294.06 Q1633.2 1296.07 1633.2 1299.44 Q1633.2 1301.04 1632.59 1302.48 Q1632 1303.9 1630.42 1305.85 Q1629.99 1306.35 1627.66 1308.76 Q1625.33 1311.16 1621.1 1315.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1637.17 1292.51 L1650.94 1292.51 L1650.94 1295.46 L1640.38 1295.46 L1640.38 1301.82 Q1641.15 1301.56 1641.91 1301.44 Q1642.68 1301.3 1643.44 1301.3 Q1647.78 1301.3 1650.32 1303.68 Q1652.85 1306.05 1652.85 1310.12 Q1652.85 1314.3 1650.25 1316.63 Q1647.64 1318.94 1642.9 1318.94 Q1641.27 1318.94 1639.57 1318.66 Q1637.88 1318.38 1636.08 1317.83 L1636.08 1314.3 Q1637.64 1315.15 1639.31 1315.57 Q1640.98 1315.98 1642.83 1315.98 Q1645.84 1315.98 1647.59 1314.41 Q1649.34 1312.83 1649.34 1310.12 Q1649.34 1307.41 1647.59 1305.83 Q1645.84 1304.25 1642.83 1304.25 Q1641.43 1304.25 1640.02 1304.56 Q1638.63 1304.87 1637.17 1305.53 L1637.17 1292.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1866.83 1304.46 Q1869.35 1305 1870.76 1306.7 Q1872.18 1308.4 1872.18 1310.9 Q1872.18 1314.73 1869.54 1316.84 Q1866.9 1318.94 1862.04 1318.94 Q1860.41 1318.94 1858.68 1318.61 Q1856.96 1318.29 1855.12 1317.65 L1855.12 1314.27 Q1856.57 1315.12 1858.31 1315.55 Q1860.05 1315.98 1861.94 1315.98 Q1865.24 1315.98 1866.96 1314.68 Q1868.69 1313.38 1868.69 1310.9 Q1868.69 1308.61 1867.08 1307.32 Q1865.48 1306.02 1862.62 1306.02 L1859.6 1306.02 L1859.6 1303.14 L1862.75 1303.14 Q1865.34 1303.14 1866.71 1302.11 Q1868.08 1301.07 1868.08 1299.13 Q1868.08 1297.13 1866.66 1296.07 Q1865.25 1295 1862.62 1295 Q1861.18 1295 1859.53 1295.31 Q1857.88 1295.62 1855.9 1296.28 L1855.9 1293.16 Q1857.89 1292.6 1859.63 1292.32 Q1861.38 1292.04 1862.93 1292.04 Q1866.92 1292.04 1869.25 1293.87 Q1871.57 1295.67 1871.57 1298.76 Q1871.57 1300.92 1870.34 1302.41 Q1869.11 1303.88 1866.83 1304.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1883.48 1294.82 Q1880.78 1294.82 1879.4 1297.5 Q1878.05 1300.15 1878.05 1305.5 Q1878.05 1310.83 1879.4 1313.5 Q1880.78 1316.16 1883.48 1316.16 Q1886.21 1316.16 1887.56 1313.5 Q1888.94 1310.83 1888.94 1305.5 Q1888.94 1300.15 1887.56 1297.5 Q1886.21 1294.82 1883.48 1294.82 M1883.48 1292.04 Q1887.84 1292.04 1890.13 1295.5 Q1892.44 1298.94 1892.44 1305.5 Q1892.44 1312.04 1890.13 1315.5 Q1887.84 1318.94 1883.48 1318.94 Q1879.13 1318.94 1876.82 1315.5 Q1874.53 1312.04 1874.53 1305.5 Q1874.53 1298.94 1876.82 1295.5 Q1879.13 1292.04 1883.48 1292.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1176.56 1345.83 L1180.96 1345.83 L1180.96 1382.98 L1176.56 1382.98 L1176.56 1345.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1197.71 1369.54 Q1192.39 1369.54 1190.34 1370.75 Q1188.29 1371.97 1188.29 1374.91 Q1188.29 1377.25 1189.81 1378.63 Q1191.37 1379.99 1194.01 1379.99 Q1197.67 1379.99 1199.86 1377.41 Q1202.08 1374.81 1202.08 1370.52 L1202.08 1369.54 L1197.71 1369.54 M1206.48 1367.72 L1206.48 1382.98 L1202.08 1382.98 L1202.08 1378.92 Q1200.58 1381.35 1198.34 1382.52 Q1196.09 1383.67 1192.85 1383.67 Q1188.74 1383.67 1186.3 1381.38 Q1183.89 1379.06 1183.89 1375.19 Q1183.89 1370.68 1186.9 1368.39 Q1189.93 1366.1 1195.92 1366.1 L1202.08 1366.1 L1202.08 1365.67 Q1202.08 1362.64 1200.08 1360.99 Q1198.1 1359.32 1194.49 1359.32 Q1192.2 1359.32 1190.03 1359.87 Q1187.86 1360.42 1185.85 1361.52 L1185.85 1357.46 Q1188.26 1356.53 1190.53 1356.07 Q1192.8 1355.6 1194.95 1355.6 Q1200.75 1355.6 1203.61 1358.6 Q1206.48 1361.61 1206.48 1367.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M1228.68 1369.3 Q1228.68 1364.52 1226.69 1361.9 Q1224.74 1359.27 1221.18 1359.27 Q1217.65 1359.27 1215.67 1361.9 Q1213.71 1364.52 1213.71 1369.3 Q1213.71 1374.05 1215.67 1376.67 Q1217.65 1379.3 1221.18 1379.3 Q1224.74 1379.3 1226.69 1376.67 Q1228.68 1374.05 1228.68 1369.3 M1233.07 1379.66 Q1233.07 1386.49 1230.04 1389.8 Q1227.01 1393.15 1220.75 1393.15 Q1218.44 1393.15 1216.38 1392.79 Q1214.33 1392.45 1212.4 1391.74 L1212.4 1387.46 Q1214.33 1388.52 1216.22 1389.02 Q1218.1 1389.52 1220.06 1389.52 Q1224.38 1389.52 1226.53 1387.25 Q1228.68 1385.01 1228.68 1380.45 L1228.68 1378.27 Q1227.32 1380.64 1225.19 1381.81 Q1223.07 1382.98 1220.11 1382.98 Q1215.19 1382.98 1212.18 1379.23 Q1209.17 1375.48 1209.17 1369.3 Q1209.17 1363.09 1212.18 1359.34 Q1215.19 1355.6 1220.11 1355.6 Q1223.07 1355.6 1225.19 1356.77 Q1227.32 1357.94 1228.68 1360.3 L1228.68 1356.24 L1233.07 1356.24 L1233.07 1379.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  445.066,1236.89 1964.57,1236.89 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  445.066,945.269 1964.57,945.269 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  445.066,653.65 1964.57,653.65 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  445.066,362.031 1964.57,362.031 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip412)\" style=\"stroke:#000000; stroke-width:1.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  445.066,70.4113 1964.57,70.4113 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,1271.33 445.066,35.4331 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,1236.89 463.3,1236.89 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,945.269 463.3,945.269 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,653.65 463.3,653.65 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,362.031 463.3,362.031 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip410)\" style=\"stroke:#000000; stroke-width:3; stroke-opacity:1; fill:none\" points=\"\n",
       "  445.066,70.4113 463.3,70.4113 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip410)\" d=\"M 0 0 M358.122 1226.24 Q355.414 1226.24 354.042 1228.91 Q352.688 1231.57 352.688 1236.91 Q352.688 1242.24 354.042 1244.92 Q355.414 1247.57 358.122 1247.57 Q360.848 1247.57 362.202 1244.92 Q363.574 1242.24 363.574 1236.91 Q363.574 1231.57 362.202 1228.91 Q360.848 1226.24 358.122 1226.24 M358.122 1223.46 Q362.48 1223.46 364.772 1226.91 Q367.081 1230.35 367.081 1236.91 Q367.081 1243.46 364.772 1246.91 Q362.48 1250.35 358.122 1250.35 Q353.765 1250.35 351.456 1246.91 Q349.164 1243.46 349.164 1236.91 Q349.164 1230.35 351.456 1226.91 Q353.765 1223.46 358.122 1223.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M370.883 1245.44 L374.546 1245.44 L374.546 1249.85 L370.883 1249.85 L370.883 1245.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M385.848 1226.24 Q383.139 1226.24 381.768 1228.91 Q380.414 1231.57 380.414 1236.91 Q380.414 1242.24 381.768 1244.92 Q383.139 1247.57 385.848 1247.57 Q388.573 1247.57 389.928 1244.92 Q391.299 1242.24 391.299 1236.91 Q391.299 1231.57 389.928 1228.91 Q388.573 1226.24 385.848 1226.24 M385.848 1223.46 Q390.205 1223.46 392.497 1226.91 Q394.806 1230.35 394.806 1236.91 Q394.806 1243.46 392.497 1246.91 Q390.205 1250.35 385.848 1250.35 Q381.49 1250.35 379.181 1246.91 Q376.89 1243.46 376.89 1236.91 Q376.89 1230.35 379.181 1226.91 Q381.49 1223.46 385.848 1223.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M406.108 1226.24 Q403.4 1226.24 402.028 1228.91 Q400.674 1231.57 400.674 1236.91 Q400.674 1242.24 402.028 1244.92 Q403.4 1247.57 406.108 1247.57 Q408.834 1247.57 410.188 1244.92 Q411.559 1242.24 411.559 1236.91 Q411.559 1231.57 410.188 1228.91 Q408.834 1226.24 406.108 1226.24 M406.108 1223.46 Q410.466 1223.46 412.757 1226.91 Q415.066 1230.35 415.066 1236.91 Q415.066 1243.46 412.757 1246.91 Q410.466 1250.35 406.108 1250.35 Q401.75 1250.35 399.441 1246.91 Q397.15 1243.46 397.15 1236.91 Q397.15 1230.35 399.441 1226.91 Q401.75 1223.46 406.108 1223.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M360.067 934.618 Q357.358 934.618 355.987 937.292 Q354.633 939.948 354.633 945.295 Q354.633 950.625 355.987 953.299 Q357.358 955.955 360.067 955.955 Q362.792 955.955 364.147 953.299 Q365.518 950.625 365.518 945.295 Q365.518 939.948 364.147 937.292 Q362.792 934.618 360.067 934.618 M360.067 931.841 Q364.424 931.841 366.716 935.296 Q369.025 938.733 369.025 945.295 Q369.025 951.841 366.716 955.295 Q364.424 958.733 360.067 958.733 Q355.709 958.733 353.4 955.295 Q351.108 951.841 351.108 945.295 Q351.108 938.733 353.4 935.296 Q355.709 931.841 360.067 931.841 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M372.827 953.82 L376.49 953.82 L376.49 958.229 L372.827 958.229 L372.827 953.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M383.313 955.278 L395.553 955.278 L395.553 958.229 L379.094 958.229 L379.094 955.278 Q381.091 953.212 384.528 949.74 Q387.983 946.25 388.869 945.243 Q390.553 943.351 391.212 942.049 Q391.889 940.73 391.889 939.462 Q391.889 937.396 390.431 936.094 Q388.99 934.792 386.664 934.792 Q385.014 934.792 383.174 935.365 Q381.351 935.938 379.268 937.101 L379.268 933.559 Q381.386 932.709 383.226 932.275 Q385.067 931.841 386.594 931.841 Q390.622 931.841 393.018 933.855 Q395.414 935.868 395.414 939.236 Q395.414 940.834 394.806 942.275 Q394.216 943.698 392.636 945.643 Q392.202 946.146 389.876 948.559 Q387.549 950.955 383.313 955.278 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M399.389 932.309 L413.157 932.309 L413.157 935.261 L402.601 935.261 L402.601 941.615 Q403.365 941.355 404.129 941.233 Q404.893 941.094 405.657 941.094 Q409.997 941.094 412.532 943.473 Q415.066 945.851 415.066 949.914 Q415.066 954.098 412.462 956.424 Q409.858 958.733 405.119 958.733 Q403.487 958.733 401.785 958.455 Q400.101 958.177 398.296 957.622 L398.296 954.098 Q399.858 954.948 401.525 955.365 Q403.191 955.782 405.049 955.782 Q408.053 955.782 409.806 954.202 Q411.559 952.622 411.559 949.914 Q411.559 947.205 409.806 945.625 Q408.053 944.045 405.049 944.045 Q403.643 944.045 402.237 944.358 Q400.848 944.67 399.389 945.33 L399.389 932.309 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M358.869 642.999 Q356.16 642.999 354.789 645.673 Q353.435 648.329 353.435 653.676 Q353.435 659.006 354.789 661.68 Q356.16 664.336 358.869 664.336 Q361.594 664.336 362.949 661.68 Q364.32 659.006 364.32 653.676 Q364.32 648.329 362.949 645.673 Q361.594 642.999 358.869 642.999 M358.869 640.221 Q363.226 640.221 365.518 643.676 Q367.827 647.114 367.827 653.676 Q367.827 660.221 365.518 663.676 Q363.226 667.114 358.869 667.114 Q354.511 667.114 352.202 663.676 Q349.91 660.221 349.91 653.676 Q349.91 647.114 352.202 643.676 Q354.511 640.221 358.869 640.221 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M371.629 662.2 L375.292 662.2 L375.292 666.61 L371.629 666.61 L371.629 662.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M379.129 640.69 L392.896 640.69 L392.896 643.641 L382.341 643.641 L382.341 649.996 Q383.105 649.735 383.869 649.614 Q384.633 649.475 385.396 649.475 Q389.737 649.475 392.271 651.853 Q394.806 654.232 394.806 658.294 Q394.806 662.478 392.202 664.804 Q389.598 667.114 384.858 667.114 Q383.226 667.114 381.525 666.836 Q379.841 666.558 378.035 666.002 L378.035 662.478 Q379.598 663.329 381.264 663.745 Q382.931 664.162 384.789 664.162 Q387.792 664.162 389.546 662.582 Q391.299 661.002 391.299 658.294 Q391.299 655.586 389.546 654.006 Q387.792 652.426 384.789 652.426 Q383.383 652.426 381.976 652.739 Q380.587 653.051 379.129 653.711 L379.129 640.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M406.108 642.999 Q403.4 642.999 402.028 645.673 Q400.674 648.329 400.674 653.676 Q400.674 659.006 402.028 661.68 Q403.4 664.336 406.108 664.336 Q408.834 664.336 410.188 661.68 Q411.559 659.006 411.559 653.676 Q411.559 648.329 410.188 645.673 Q408.834 642.999 406.108 642.999 M406.108 640.221 Q410.466 640.221 412.757 643.676 Q415.066 647.114 415.066 653.676 Q415.066 660.221 412.757 663.676 Q410.466 667.114 406.108 667.114 Q401.75 667.114 399.441 663.676 Q397.15 660.221 397.15 653.676 Q397.15 647.114 399.441 643.676 Q401.75 640.221 406.108 640.221 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M359.546 351.38 Q356.838 351.38 355.466 354.053 Q354.112 356.709 354.112 362.057 Q354.112 367.387 355.466 370.06 Q356.838 372.716 359.546 372.716 Q362.272 372.716 363.626 370.06 Q364.997 367.387 364.997 362.057 Q364.997 356.709 363.626 354.053 Q362.272 351.38 359.546 351.38 M359.546 348.602 Q363.903 348.602 366.195 352.057 Q368.504 355.494 368.504 362.057 Q368.504 368.602 366.195 372.057 Q363.903 375.494 359.546 375.494 Q355.188 375.494 352.879 372.057 Q350.588 368.602 350.588 362.057 Q350.588 355.494 352.879 352.057 Q355.188 348.602 359.546 348.602 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M372.306 370.581 L375.969 370.581 L375.969 374.991 L372.306 374.991 L372.306 370.581 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M378.886 349.071 L395.553 349.071 L395.553 350.564 L386.143 374.991 L382.48 374.991 L391.334 352.022 L378.886 352.022 L378.886 349.071 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M399.389 349.071 L413.157 349.071 L413.157 352.022 L402.601 352.022 L402.601 358.376 Q403.365 358.116 404.129 357.994 Q404.893 357.855 405.657 357.855 Q409.997 357.855 412.532 360.234 Q415.066 362.612 415.066 366.675 Q415.066 370.859 412.462 373.185 Q409.858 375.494 405.119 375.494 Q403.487 375.494 401.785 375.216 Q400.101 374.939 398.296 374.383 L398.296 370.859 Q399.858 371.709 401.525 372.126 Q403.191 372.543 405.049 372.543 Q408.053 372.543 409.806 370.963 Q411.559 369.383 411.559 366.675 Q411.559 363.966 409.806 362.387 Q408.053 360.807 405.049 360.807 Q403.643 360.807 402.237 361.119 Q400.848 361.432 399.389 362.091 L399.389 349.071 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M352.15 80.4199 L357.879 80.4199 L357.879 60.6457 L351.647 61.8957 L351.647 58.7012 L357.844 57.4513 L361.351 57.4513 L361.351 80.4199 L367.081 80.4199 L367.081 83.3713 L352.15 83.3713 L352.15 80.4199 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M370.883 78.9616 L374.546 78.9616 L374.546 83.3713 L370.883 83.3713 L370.883 78.9616 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M385.848 59.7603 Q383.139 59.7603 381.768 62.4339 Q380.414 65.0901 380.414 70.4373 Q380.414 75.7671 381.768 78.4407 Q383.139 81.097 385.848 81.097 Q388.573 81.097 389.928 78.4407 Q391.299 75.7671 391.299 70.4373 Q391.299 65.0901 389.928 62.4339 Q388.573 59.7603 385.848 59.7603 M385.848 56.9825 Q390.205 56.9825 392.497 60.4373 Q394.806 63.8748 394.806 70.4373 Q394.806 76.9824 392.497 80.4372 Q390.205 83.8747 385.848 83.8747 Q381.49 83.8747 379.181 80.4372 Q376.89 76.9824 376.89 70.4373 Q376.89 63.8748 379.181 60.4373 Q381.49 56.9825 385.848 56.9825 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M406.108 59.7603 Q403.4 59.7603 402.028 62.4339 Q400.674 65.0901 400.674 70.4373 Q400.674 75.7671 402.028 78.4407 Q403.4 81.097 406.108 81.097 Q408.834 81.097 410.188 78.4407 Q411.559 75.7671 411.559 70.4373 Q411.559 65.0901 410.188 62.4339 Q408.834 59.7603 406.108 59.7603 M406.108 56.9825 Q410.466 56.9825 412.757 60.4373 Q415.066 63.8748 415.066 70.4373 Q415.066 76.9824 412.757 80.4372 Q410.466 83.8747 406.108 83.8747 Q401.75 83.8747 399.441 80.4372 Q397.15 76.9824 397.15 70.4373 Q397.15 63.8748 399.441 60.4373 Q401.75 56.9825 406.108 56.9825 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M277.582 805.478 L295.319 812.018 L295.319 798.913 L277.582 805.478 M272.832 808.199 L272.832 802.732 L308.472 789.15 L308.472 794.163 L299.329 797.409 L299.329 813.475 L308.472 816.721 L308.472 821.806 L272.832 808.199 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M297.921 784.996 L281.736 784.996 L281.736 780.604 L297.754 780.604 Q301.549 780.604 303.459 779.124 Q305.345 777.644 305.345 774.683 Q305.345 771.127 303.077 769.074 Q300.809 766.997 296.894 766.997 L281.736 766.997 L281.736 762.605 L308.472 762.605 L308.472 766.997 L304.366 766.997 Q306.801 768.596 307.995 770.721 Q309.164 772.822 309.164 775.614 Q309.164 780.222 306.3 782.609 Q303.435 784.996 297.921 784.996 M281.091 773.943 L281.091 773.943 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M274.145 753.653 L281.736 753.653 L281.736 744.606 L285.15 744.606 L285.15 753.653 L299.663 753.653 Q302.934 753.653 303.865 752.77 Q304.796 751.862 304.796 749.117 L304.796 744.606 L308.472 744.606 L308.472 749.117 Q308.472 754.202 306.586 756.135 Q304.676 758.069 299.663 758.069 L285.15 758.069 L285.15 761.292 L281.736 761.292 L281.736 758.069 L274.145 758.069 L274.145 753.653 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M284.815 729.638 Q284.815 733.171 287.584 735.224 Q290.33 737.277 295.128 737.277 Q299.926 737.277 302.695 735.248 Q305.44 733.195 305.44 729.638 Q305.44 726.129 302.671 724.076 Q299.902 722.023 295.128 722.023 Q290.377 722.023 287.608 724.076 Q284.815 726.129 284.815 729.638 M281.091 729.638 Q281.091 723.909 284.815 720.639 Q288.539 717.368 295.128 717.368 Q301.692 717.368 305.44 720.639 Q309.164 723.909 309.164 729.638 Q309.164 735.391 305.44 738.662 Q301.692 741.908 295.128 741.908 Q288.539 741.908 284.815 738.662 Q281.091 735.391 281.091 729.638 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M282.762 693.521 L286.868 693.521 Q285.842 695.383 285.341 697.269 Q284.815 699.131 284.815 701.04 Q284.815 705.313 287.537 707.676 Q290.234 710.04 295.128 710.04 Q300.021 710.04 302.743 707.676 Q305.44 705.313 305.44 701.04 Q305.44 699.131 304.939 697.269 Q304.414 695.383 303.387 693.521 L307.445 693.521 Q308.305 695.359 308.735 697.34 Q309.164 699.298 309.164 701.518 Q309.164 707.557 305.369 711.114 Q301.573 714.671 295.128 714.671 Q288.587 714.671 284.839 711.09 Q281.091 707.486 281.091 701.231 Q281.091 699.202 281.521 697.269 Q281.927 695.335 282.762 693.521 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M284.815 678.553 Q284.815 682.086 287.584 684.139 Q290.33 686.192 295.128 686.192 Q299.926 686.192 302.695 684.163 Q305.44 682.11 305.44 678.553 Q305.44 675.044 302.671 672.991 Q299.902 670.938 295.128 670.938 Q290.377 670.938 287.608 672.991 Q284.815 675.044 284.815 678.553 M281.091 678.553 Q281.091 672.824 284.815 669.554 Q288.539 666.283 295.128 666.283 Q301.692 666.283 305.44 669.554 Q309.164 672.824 309.164 678.553 Q309.164 684.306 305.44 687.577 Q301.692 690.823 295.128 690.823 Q288.539 690.823 284.815 687.577 Q281.091 684.306 281.091 678.553 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M285.842 646.184 Q285.412 646.924 285.221 647.807 Q285.006 648.666 285.006 649.717 Q285.006 653.441 287.441 655.446 Q289.852 657.427 294.388 657.427 L308.472 657.427 L308.472 661.843 L281.736 661.843 L281.736 657.427 L285.89 657.427 Q283.455 656.043 282.285 653.823 Q281.091 651.603 281.091 648.428 Q281.091 647.974 281.163 647.425 Q281.211 646.876 281.33 646.208 L285.842 646.184 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M285.842 626.943 Q285.412 627.683 285.221 628.567 Q285.006 629.426 285.006 630.476 Q285.006 634.2 287.441 636.206 Q289.852 638.187 294.388 638.187 L308.472 638.187 L308.472 642.603 L281.736 642.603 L281.736 638.187 L285.89 638.187 Q283.455 636.802 282.285 634.582 Q281.091 632.362 281.091 629.187 Q281.091 628.734 281.163 628.185 Q281.211 627.636 281.33 626.967 L285.842 626.943 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M294.006 600.542 L296.154 600.542 L296.154 620.737 Q300.69 620.45 303.077 618.015 Q305.44 615.557 305.44 611.188 Q305.44 608.658 304.82 606.295 Q304.199 603.908 302.958 601.568 L307.111 601.568 Q308.114 603.931 308.639 606.414 Q309.164 608.897 309.164 611.451 Q309.164 617.848 305.44 621.596 Q301.716 625.32 295.367 625.32 Q288.802 625.32 284.959 621.787 Q281.091 618.23 281.091 612.215 Q281.091 606.82 284.577 603.693 Q288.038 600.542 294.006 600.542 M292.717 604.934 Q289.112 604.982 286.964 606.963 Q284.815 608.92 284.815 612.167 Q284.815 615.843 286.892 618.063 Q288.969 620.259 292.741 620.594 L292.717 604.934 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M271.328 595.934 L271.328 591.542 L308.472 591.542 L308.472 595.934 L271.328 595.934 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M295.032 574.784 Q295.032 580.108 296.25 582.161 Q297.467 584.214 300.403 584.214 Q302.743 584.214 304.127 582.686 Q305.488 581.134 305.488 578.484 Q305.488 574.832 302.91 572.636 Q300.308 570.416 296.011 570.416 L295.032 570.416 L295.032 574.784 M293.218 566.024 L308.472 566.024 L308.472 570.416 L304.414 570.416 Q306.849 571.92 308.018 574.164 Q309.164 576.408 309.164 579.654 Q309.164 583.76 306.873 586.195 Q304.557 588.606 300.69 588.606 Q296.178 588.606 293.887 585.598 Q291.595 582.566 291.595 576.575 L291.595 570.416 L291.165 570.416 Q288.134 570.416 286.486 572.421 Q284.815 574.402 284.815 578.007 Q284.815 580.299 285.364 582.471 Q285.913 584.643 287.012 586.648 L282.953 586.648 Q282.022 584.237 281.569 581.97 Q281.091 579.702 281.091 577.553 Q281.091 571.753 284.099 568.888 Q287.107 566.024 293.218 566.024 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M274.145 557.072 L281.736 557.072 L281.736 548.025 L285.15 548.025 L285.15 557.072 L299.663 557.072 Q302.934 557.072 303.865 556.189 Q304.796 555.281 304.796 552.536 L304.796 548.025 L308.472 548.025 L308.472 552.536 Q308.472 557.621 306.586 559.554 Q304.676 561.488 299.663 561.488 L285.15 561.488 L285.15 564.711 L281.736 564.711 L281.736 561.488 L274.145 561.488 L274.145 557.072 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M281.736 543.417 L281.736 539.025 L308.472 539.025 L308.472 543.417 L281.736 543.417 M271.328 543.417 L271.328 539.025 L276.89 539.025 L276.89 543.417 L271.328 543.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M284.815 524.058 Q284.815 527.591 287.584 529.644 Q290.33 531.697 295.128 531.697 Q299.926 531.697 302.695 529.667 Q305.44 527.615 305.44 524.058 Q305.44 520.549 302.671 518.496 Q299.902 516.443 295.128 516.443 Q290.377 516.443 287.608 518.496 Q284.815 520.549 284.815 524.058 M281.091 524.058 Q281.091 518.329 284.815 515.058 Q288.539 511.788 295.128 511.788 Q301.692 511.788 305.44 515.058 Q309.164 518.329 309.164 524.058 Q309.164 529.811 305.44 533.081 Q301.692 536.328 295.128 536.328 Q288.539 536.328 284.815 533.081 Q281.091 529.811 281.091 524.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip410)\" d=\"M 0 0 M292.335 484.956 L308.472 484.956 L308.472 489.349 L292.478 489.349 Q288.683 489.349 286.797 490.829 Q284.911 492.309 284.911 495.269 Q284.911 498.826 287.179 500.879 Q289.446 502.931 293.361 502.931 L308.472 502.931 L308.472 507.348 L281.736 507.348 L281.736 502.931 L285.89 502.931 Q283.479 501.356 282.285 499.231 Q281.091 497.083 281.091 494.29 Q281.091 489.683 283.956 487.32 Q286.797 484.956 292.335 484.956 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip412)\" style=\"stroke:#009af9; stroke-width:9; stroke-opacity:1; fill:none\" points=\"\n",
       "  488.071,70.4113 535.854,320.72 583.637,517.32 631.42,671.736 679.203,793.021 726.986,888.283 774.769,963.106 822.552,1021.88 870.335,1068.04 918.118,1104.3 \n",
       "  965.901,1132.78 1013.68,1155.15 1061.47,1172.72 1109.25,1186.52 1157.03,1197.37 1204.82,1205.89 1252.6,1212.58 1300.38,1217.84 1348.17,1221.97 1395.95,1225.22 \n",
       "  1443.73,1227.77 1491.51,1229.78 1539.3,1231.36 1587.08,1232.6 1634.86,1233.58 1682.65,1234.35 1730.43,1234.96 1778.21,1235.44 1826,1235.81 1873.78,1236.11 \n",
       "  1921.56,1236.35 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(1:length(A), A, ylabel=\"Autocorrelation\", xlabel=\"lag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like after 10 periods the empirical autocorrelation is quite small but still not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed b asymptotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neave, 1970]: \"*When proving results on the asymptotic behavior of estimates of the spectrum of a stationary time series, it is invariably assumed that as the sample size $n$ tends to infinity, so does the truncation point $h$, but at a slower rate, so that $\\frac{h}{n}$ tends to zero. This is a convenient assumption mathematically in that, in particular, it ensures consistency of the estimates, but it is unrealistic when such results are used as approximations to the finite case where the value of $\\frac{h}{n}$ cannot be zero.*\"\"\n",
    "\n",
    "[Kiefer and Vogelsang, 2005]: \"*if you make $h$ large (even $h=n$), as a result $\\hat{\\Omega}^{HAC} _ {k,h} - \\Omega_n \\not \\overset{p}{\\to} 0$. In general, it happens whenever $\\frac{h}{n} \\to b > 0$ for any fixed $b$. Nonetheless, the construction of correct confidence intervals is possible.*\"\n",
    "\n",
    "**Theorem**:\n",
    "Under regularity conditions,\n",
    "$$\n",
    "\t\\sqrt{n} \\Big( V^{HAC} _ {k,h} \\Big)(\\hat \\beta - \\beta_0) \\overset{d}{\\to} F \\\\\n",
    "$$\n",
    "\n",
    "The asymptotic critical values of the $F$ statistic depend on the choice of the kernel. In order to do hypothesis testing, Kiefer and Vogelsang(2005) provide critical value functions for the t-statistic for each kernel-confidence level combination using a cubic equation:\n",
    "$$\n",
    "\tcv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3\n",
    "$$\n",
    "\n",
    "Example for the Bartlett kernel:\n",
    "\n",
    "<img src=\"figures/Fig_332.png\" alt=\"Fixed-b\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed G asymptotics\n",
    "\n",
    "[Bester, 2013]: \"*Cluster covariance estimators are routinely used with data that has a group structure with independence assumed across groups. Typically, inference is conducted in such settings under the assumption that there are a large number of these independent groups.*\"\"\n",
    "\n",
    "\"*However, with enough weakly dependent data, we show that groups can be chosen by the researcher so that group-level averages are approximately independent. Intuitively, if groups are large enough and well shaped (e.g. do not have gaps), the majority of points in a group will be far from other groups, and hence approximately independent of observations from other groups provided the data are weakly dependent. The key prerequisite for our methods is the researcher's ability to construct groups whose averages are approximately independent. As we show later, this often requires that the number of groups be kept relatively small, which is why our main results explicitly consider a fixed (small) number of groups.*\"\"\n",
    "\n",
    "**Assumption**:\n",
    "Suppose you have data $D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where $y _ {it} = x _ {it}' \\beta + \\alpha_i + \\varepsilon _ {it}$ where $i$ indexes the observational unit and $t$ indexes time (could also be space).\n",
    "\n",
    "Let\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  & \\tilde{y} _ {it} = y _ {it} - \\frac{1}{T} \\sum _ {t=1}^T y _ {it} \\\\\n",
    "  & \\tilde{x} _ {it} = x _ {it} - \\frac{1}{T} \\sum _ {t=1}^T x _ {it} \\\\\n",
    "  & \\tilde{\\varepsilon} _ {it} = \\varepsilon _ {it} - \\frac{1}{T} \\sum _ {t=1}^T \\varepsilon _ {it}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\tilde{y} _ {it} = \\tilde{x} _ {it}' \\beta + \\tilde{\\varepsilon} _ {it}\n",
    "$$\n",
    "\n",
    "The $\\tilde{\\varepsilon} _ {it}$ are by construction correlated between each other even if the original $\\varepsilon$ was iid. The **cluster score variance estimator** is given by:\n",
    "$$\n",
    "  \\hat{\\Omega}^{CL} = \\frac{1}{T-1} \\sum _ {i=1}^n  \\sum _ {t=1}^T  \\sum _ {s=1}^T \\tilde{x} _ {it} \\hat{\\tilde{\\varepsilon}} _ {it} \\tilde{x} _ {is}     \\hat{\\tilde{\\varepsilon}} _ {is}\n",
    "$$\n",
    "\n",
    "> It's very similar too the HAC estimator since we have *dependent cross-products* here as well. However, here we do not consider the $i \\times j$ cross-products. We only have time-dependency (state).\n",
    "\n",
    "On $T$ and $n$:\n",
    "\t\n",
    "- If $T$ is fixed and $n \\to \\infty$, then the number of cross-products considered is much smaller than the total number of cross-products.\n",
    "- If $T >> n$ issues arise since the number of cross products considered is close to the total number of cross products. As in HAC estimation, this is a problem because it implies that the algebraic estimate of the cluster score variance gets close to zero because of the orthogonality property of the residuals.\n",
    "- The panel assumption is that observations across individuals are not correlated.\n",
    "\n",
    "> Strategy: as in HAC, we want to limit the correlation across clusters (individuals). We hope that observations are **negligibly dependent** between cluster sufficiently distant from each other.\n",
    "\n",
    "Classical cluster robust estimator:\n",
    "$$\n",
    "  \\hat{\\Omega}^{CL} = \\frac{1}{n} \\sum _ {i=1}^n x_i \\varepsilon_i x_j' \\varepsilon_j' \\mathbb{I}   \\\\{ i,j \\text{ in the same cluster} \\\\}\n",
    "$$\n",
    "\n",
    "> On clusters:\n",
    ">\t\n",
    "- If the number of observations near a boundary is small relative to the sample size, ignoring the dependence should not affect inference too adversely.\n",
    "- The higher the dimension of the data, the easier it is to have observations near boundaries (*curse of dimensionality*).\n",
    "- We would like to have few clusters in order to make less independence assumptions. However, few clusters means bigger blocks and hence a larger number of cross-products to estimate. If the number of cross-products is too large (relative to the sample size), $\\hat{\\Omega}^{CL}$ does not converge\n",
    "\n",
    "**Theorem**:\n",
    "Under regularity conditions:\n",
    "$$\n",
    "\t\\hat{t} \\overset{d}{\\to} \\sqrt{\\frac{G}{G-1}} t _ {G-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.07689815166053209\n",
       " 0.06893406353323414"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Homoskedastic standard errors\n",
    "std_h = (var(e_hat) * inv(X'*X))[[1,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.27848518384485854\n",
       " 0.22996611098764827"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HC0 variance and standard errors\n",
    "omega_hc0 = X' * Diagonal(e_hat.^2) * X;\n",
    "std_hc0 = sqrt.((inv(X'*X) * omega_hc0 * inv(X'*X))[[1,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.2813125170809742\n",
       " 0.2323008521749491"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HC1 variance and standard errors\n",
    "omega_hc1 = n/(n-k) * X' *  Diagonal(e_hat.^2) * X;\n",
    "std_hc1 = sqrt.((inv(X'*X) * omega_hc1 * inv(X'*X))[[1,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.28317273173517976\n",
       " 0.23391064716815493"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HC2 variance and standard errors\n",
    "omega_hc2 = X' * Diagonal(e_hat.^2 ./ (1 .- h)) * X;\n",
    "std_hc2 = sqrt.((inv(X'*X) * omega_hc2 * inv(X'*X))[[1,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.28798503309542056\n",
       " 0.23795266595177875"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HC3 variance and standard errors\n",
    "omega_hc3 = X' * Diagonal(e_hat.^2 ./ (1 .- h).^2) * X;\n",
    "std_hc3 = sqrt.((inv(X'*X) * omega_hc3 * inv(X'*X))[[1,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       "  2.89906e-29  -1.0058e-28\n",
       " -1.15963e-28   3.37632e-28"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note what happens if you allow for full autocorrelation\n",
    "omega_full = X'*e_hat*e_hat'*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do inference on $\\hat \\beta$ we need to know its distribution. We have two options: (i) assume gaussian error term (extended GM) or (ii) rely on asymptotic approximations (CLT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical hypothesis is a subset of a statistical model, $\\mathcal K \\subset \\mathcal F$.  A hypothesis test is a map $\\mathcal D \\rightarrow \\{ 0,1 \\}$, $D \\mapsto T$.  If $\\mathcal F$ is the statistical model and $\\mathcal K$ is the statistical hypothesis, we use the notation $H_0: \\Pr \\in \\mathcal K$.\n",
    "\n",
    "> Generally, we are interested in understanding whether it is likely that data $D$ are drawn from $\\mathcal K$ or not.\n",
    "\n",
    "A hypothesis test, $T$ is our tool for deciding whether the hypothesis is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and test inconclusive  $T(D)=1$ $\\implies$ reject $H_0$ and $D$ is inconsistent with any $\\Pr \\in \\mathcal K$.\n",
    "\n",
    "Let $\\mathcal K \\subseteq \\mathcal F$ be a statistical hypothesis and $T$ a hypothesis test.  \n",
    "\t \n",
    "1. Suppose $\\Pr \\in \\mathcal K$.  A Type I error (relative to $\\Pr$) is an event $T(D)=1$ under $\\Pr$.  \n",
    "2. Suppose $\\Pr \\in \\mathcal K^c$.  A Type II error (relative to $\\Pr$) is an event $T(D)=0$ under $\\Pr$.\n",
    "\t\n",
    "\n",
    "The corresponding probability of a type I error is called **size**.  The corresponding probability of a type II error is called **power** (against the alternative $\\Pr$). \n",
    "\n",
    "In this section, we are interested in testing three hypotheses, under the assumptions of linearity, strict exogeneity, no multicollinearity, normality on the error term.  They are: \n",
    "\n",
    "1. $H_0: \\beta _ {0k} = \\bar \\beta _ {0k}$ (single coefficient, $\\bar \\beta _ {0k} \\in \\mathbb R$, $k \\leq K$)\n",
    "2. $a' \\beta_0 = c$  (linear combination, $a \\in \\mathbb R^K, c \\in \\mathbb R$)\n",
    "3. $R \\beta_0 = r$ (linear restrictions, $R \\in \\mathbb R^{p \\times K}$, full rank, $r \\in \\mathbb R^p$) \\\\\n",
    "\n",
    "Consider the testing problem\n",
    "$H_0: \\beta _ {0k} = \\bar \\beta _ {0k}$ where $\\bar \\beta _ {0k}$ is a pre-specified value under the null.\n",
    "The t-statistic for this problem is defined by\n",
    "$$\n",
    "  t_k:= \\frac{b_k - \\bar \\beta _ {0k}}{SE(b_k)}, \\ \\ SE(b_k):= \\sqrt{s^2 [(X'X)^{-1}] _ {kk}}\n",
    "$$\n",
    "\n",
    "**Theorem**:  In the testing procedure above, the sampling distribution under the null $H_0$ is given by\n",
    "$$\n",
    "\tt_k|X \\sim t _ {n-k} \\ \\ \\text{and so} \\ \\ t_k \\sim t _ {n-k} \\\\\n",
    "$$\n",
    "\n",
    "$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of freedom. The test can be one sided or two sided.  The above sampling distribution can be used to construct a confidence interval.\n",
    "\n",
    "**Example**:\n",
    "We want to asses whether or not the ``true\" coefficient $\\beta_0$ equals a specific value $\\hat \\beta$. Specifically, we are interested in testing $H_0$ against $H_1$, where:\n",
    "\t\n",
    "- *Null Hypothesis*: $H_0: \\beta_0 = \\hat \\beta$\n",
    "- *Alternative Hypothesis*: $H_1: \\beta_0 \\ne \\hat \\beta$.\n",
    "\n",
    "Hence, we are interested in a statistic informative about $H_1$, which is the Wald test statistic\n",
    "$$\n",
    "\t|T^*| = \\bigg| \\frac{\\hat \\beta - \\beta_0}{\\sigma(\\hat \\beta)}\\bigg|  \\sim N(0,1)\n",
    "$$\n",
    "\n",
    "However, the true variance $\\sigma^2(\\hat \\beta )$ is not known and has to be estimated. Therefore we plug in the sample variance $\\hat \\sigma^2(\\hat \\beta) = \\frac{n}{n-1} \\mathbb E_n[\\hat e_i^2]$ and we use\n",
    "$$\n",
    "\t|T| = \\bigg| \\frac{\\hat \\beta - \\beta_0}{\\hat \\sigma (\\hat \\beta)}\\bigg|  \\sim t _ {(n-k)}\n",
    "$$\n",
    "\n",
    "Hypothesis testing is like proof by contradiction. Imagine the sampling distribution was generated by $\\beta$. If it is highly improbable to observe $\\hat \\beta$ given $\\beta_0 = \\beta$ then we reject the hypothesis that the sampling distribution was generated by $\\beta$. \n",
    "\n",
    "Then, given a realized value of the statistic $|T|$, we take the following decision:\n",
    "\n",
    "- *Do not reject $H_0$*: it is consistent with random variation under true $H_0$---i.e., $|T|$ small as it has an exact student t distribution with $(n-k)$ degree of freedom in the normal regression model.\n",
    "- *Reject $H_0$ in favor of $H_1$*: $|T| > c$, with $c$ being the critical values selected to control for false rejections: $\\Pr(|t _ {n-k}| \\geq c) = \\alpha$. Moreover, you can also reject $H_0$ if the p-value $p$ is such that: $p < \\alpha$.\n",
    "\n",
    "The probability of false rejection is decreasing in $c$, i.e. the critical value for a given significant level.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\Pr (\\text{Reject } H_0 | H_0) \t& = \\Pr (|T|> c | H_0 ) = \\\\\n",
    "\t& = \\Pr (T > c | H_0 ) + \t \\Pr (T < -c | H_0 ) = \\\\\n",
    "\t& = 1 - F(c) + F(-c) = 2(1-F(c))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Example**:\n",
    "Consider the testing problem $H_0: a'\\beta_0=c$ where $a$ is a pre-specified linear combination under study. The t-statistic for this problem is defined by: \n",
    "$$\n",
    "  t_k:= \\frac{a'b - c}{SE(a'b)}, \\ \\ SE(a'b):= \\sqrt{s^2 a'(X'X)^{-1}a}\n",
    "$$\n",
    "\n",
    "**Theorem**: \n",
    "In the testing procedure above, the sampling distribution under the null $H_0$ is given by\n",
    "$$\n",
    "\tt_a|X \\sim t _ {n-K} \\quad\\text{and so} \\quad t_a \\sim t _ {n-K} \n",
    "$$\n",
    "\n",
    "Like in the previous test, $t _ {(n-K)}$ denotes the t-distribution with $(n-K)$ degress of freedom. The test can again be one sided or two sided.  The above sampling distribution can be used to construct a confidence interval.\n",
    "\n",
    "**Example**:\n",
    "Consider the testing problem \n",
    "$$\n",
    "\tH_0: R \\beta_0 = r\n",
    "$$\n",
    "where $R \\in \\mathbb R^{p \\times k}$ is a presepecified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector.\n",
    "\n",
    "The  F-statistic  for this problem is given by\n",
    "$$\n",
    "\tF:= \\frac{(Rb-r)'[R(X'X)R']^{-1}(Rb-r)/p }{s^2}\n",
    "$$\n",
    "\n",
    "**Theorem**:  \n",
    "For the problem, the sampling distribution of the F-statistic under the null $H_0:$\n",
    "$$\n",
    "\tF|X \\sim F _ {p,n-K} \\ \\ \\text{and so} \\ \\ F \\sim F _ {p,n-K} \\\\\n",
    "$$\n",
    "\n",
    "The test is intrinsically two-sided.  The above sampling distribution can be used to construct a confidence interval.\n",
    "\n",
    "**Theorem**:  \n",
    "Consider the testing problem $H_0: R \\beta_0 = r$ where $R \\in \\mathbb R^{p\\times K}$ is a presepecified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector.\n",
    "\n",
    "Consider the restricted least squares estimator, denoted $\\hat \\beta_R$: $\\hat \\beta_R: = \\text{arg} \\min _ { \\beta: R  \\beta = r } Q( \\beta)$. Let $SSR_U = Q(b), \\ \\ SSR_R=Q(\\hat \\beta_R)$. Then the $F$ statistic is numerically equivalent to the following expression: $F = \\frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confidence interval at $(1-\\alpha)$** is a random set $C$ such that \n",
    "$$\n",
    "\t \\Pr(\\beta_0 \\in C) \\geq 1- \\alpha\n",
    "$$\n",
    "i.e. the probability that $C$ covers the true value $\\beta$ is fixed at $(1-\\alpha)$.\n",
    "\n",
    "Since $C$ is not known, it has to be estimated ($\\hat{C}$). We construct confidence intervals such that:\n",
    "\n",
    "- they are symmetric around $\\hat \\beta$;\n",
    "- their length is proportional to $\\sigma(\\hat \\beta) = \\sqrt{Var(\\hat \\beta)}$.\n",
    "\n",
    "\n",
    "A CI is equivalent to the set of parameter values such that the t-statistic is less than $c$, i.e.,\n",
    "$$\n",
    "\\hat{C} = \\bigg\\{ \\beta: |T(\\beta) | \\leq c \\bigg\\} = \\bigg\\{ \\beta: - c\\leq \\frac{\\beta - \\hat \\beta}{\\sigma(\\hat \\beta)} \\leq c \\bigg\\}\n",
    "$$\n",
    "\n",
    "In practice, to construct a 95% confidence interval for a single coefficient estimate $\\hat \\beta_j$, we use the fact that\n",
    "$$\n",
    "\t\\Pr \\left( \\frac{| \\hat \\beta_j - \\beta _ {0,j} |}{ \\sqrt{\\sigma^2 [(X'X)^{-1}] _ {jj} }} > 1.96 \\right) = 0.05\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 6.986930018947694\n",
       " 5.581506996134896"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t-test for beta=0\n",
    "t = abs.(b_hat./(std_hc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 1.404876215360673e-12\n",
       " 1.1922173026590599e-8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p-value\n",
    "p_val = 1 .- normcdf.(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.930368338411945"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F statistic of joint significance\n",
    "SSR_u = e_hat'*e_hat;\n",
    "SSR_r = y'*y;\n",
    "F = (SSR_r - SSR_u)/k / (SSR_u/(n-k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Vector{Float64}}:\n",
       " [1.4141383368200853, -1.7518985018854771]\n",
       " [2.516883403777504, -0.8412791613596766]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 95% confidente intervals\n",
    "conf_int = [b_hat - 1.96*std_hc1, b_hat + 1.96*std_hc1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kozbur (2019). PhD Econometrics - Lecture Notes.\n",
    "- Hansen (2019). \"*Econometrics*\".\n",
    "- Kiefer and Vogelsang (2005). \"*A new asymptotic theory for heteroskedasticity-autocorrelation robust tests*\".\n",
    "- Wooldridge (2010). \"*Econometric Analysis of Cross Section and Panel Data*\". \n",
    "- Greene (2006). \"*Econometric Analysis*\". \n",
    "- Hayiashi (2000). \"*Econometrics*\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
