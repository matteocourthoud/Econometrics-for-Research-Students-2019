{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [Probability](#Probability)\n",
    " - [Random Variables](#Random-Variables)\n",
    " - [Moments](#Moments)\n",
    " - [Inequalities](#Inequalities)\n",
    " - [Main Theorems](#Main-Theorems)\n",
    " - [Statistical Models](#Statistical-Models)\n",
    " - [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **probability space** is a triple $(\\Omega, \\mathcal A, P)$ where\n",
    "\n",
    "- $\\Omega$ is the sample space.\n",
    "- $\\mathcal A$ is the $\\sigma$-algebra on $\\Omega$.\n",
    "- $P$ is a probability measure.\n",
    "\n",
    "The **sample space** $\\Omega$ is the space of all possible events.\n",
    "\n",
    "A nonempty set (of subsets of $\\Omega$) $\\mathcal A \\in 2^\\Omega$ is a **sigma algebra** ($\\sigma$-algebra) of $\\Omega$ if the following conditions hold:\n",
    "\n",
    "1. $\\Omega \\in \\mathcal A$\n",
    "2. If $A \\in \\mathcal A$, then $(\\Omega - A) \\in \\mathcal A$\n",
    "3. If $A_1, A_2, ... \\in \\mathcal A$, then $\\bigcup _ {i=1}^{\\infty} A_i \\in \\mathcal A$\n",
    "\n",
    "> The smallest $\\sigma$-algebra is $\\{ \\emptyset, \\Omega \\}$ and the largest one is $2^\\Omega$ (in cardinality terms).\n",
    "\n",
    "Suppose $\\Omega = \\mathbb R$. Let $\\mathcal{C} = \\{ (a, b],-\\infty \\leq a<b<\\infty \\}$. Then the **Borel** $\\sigma$**- algebra** on $\\mathbb R$ is defined by\n",
    "$$\n",
    "  \\mathcal B (\\mathbb R) = \\sigma (\\mathcal C)\n",
    "$$\n",
    "\n",
    "A **probability measure** $P$ is a set function with domain $\\mathcal A$ and codomain $[0,1]$ such that\n",
    "\n",
    "1. $P(A) \\geq 0 \\ \\forall A \\in \\mathcal A$\n",
    "2. $P$ is $\\sigma$-additive: is $A_n \\in \\mathcal A$ are pairwise disjoint events ($A_j \\cap A_k = \\emptyset$ for $j \\neq k$), then\n",
    "\n",
    "$$\n",
    "  P\\left(\\bigcup _ {n=1}^{\\infty} A_{n} \\right)=\\sum _ {n=1}^{\\infty} P\\left(A_{n}\\right)\n",
    "$$\n",
    "\n",
    "3. $P(\\Omega) = 1$\n",
    "\n",
    "> Properties\n",
    ">\n",
    ">- $P\\left(A^{c}\\right)=1-P(A)$\n",
    ">- $P(\\emptyset)=0$\n",
    ">- For $A, B \\in \\mathcal{A}$, $P(A \\cup B)=P(A)+P(B)-P(A \\cap B)$\n",
    ">- For $A, B \\in \\mathcal{A}$, if $A \\subset B$ then $P(A) \\leq P(B)$\n",
    ">- For $A_n \\in \\mathcal{A}$, $P \\left(\\cup _ {n=1}^\\infty A_{n} \\right) \\leq \\sum _ {n=1}^\\infty P(A_n)$\n",
    ">- For $A_n \\in \\mathcal{A}$, if $A_n \\uparrow A$ then $\\lim _ {n \\to \\infty} P(A_n) = P(A)$\n",
    "\n",
    "Let $A, B \\in \\mathcal A$ and $P(B) > 0$, the **conditional probability** of $A$ given $B$ is\n",
    "\n",
    "$$\n",
    "  P(A | B)=\\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Two events $A$ and $B$ are **independent** if $P(A \\cap B)=P(A) P(B)$.\n",
    "\n",
    "**Theorem** (Law of Total Probability):\n",
    "Let $(E_n) _ {n \\geq 1}$ be a finite or countable partition of $\\Omega$. Then, if $A \\in \\mathcal A$, \n",
    "\n",
    "$$\n",
    "  P(A) = \\sum_n P(A | E_n ) P(E_n)\n",
    "$$\n",
    "\n",
    "**Theorem** (Bayes Theorem):\n",
    "Let $(E_n) _ {n \\geq 1}$ be a finite or countable partition of $\\Omega$, and suppose $P(A) > 0$. Then, \n",
    "\n",
    "$$\n",
    "  P(E_n | A) = \\frac{P(A | E_n) P(E_n)}{\\sum_m P(A | E_m) P(E_m)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **random variable** $X$ on a probability space $(\\Omega,\\mathcal A, P)$ is a (measurable) mapping $X : \\Omega \\to \\mathbb{R}$ such that\n",
    "\n",
    "$$\n",
    "  \\forall B \\in \\mathcal{B}(\\mathbb{R}), \\quad X^{-1}(B) \\in \\mathcal{A}\n",
    "$$\n",
    "\n",
    "> The measurability condition states that the inverse image is a measurable set of $\\Omega$ i.e. $X^{-1}(B) \\in \\mathcal A$. This is essential since probabilities are defined only on $\\mathcal A$.\n",
    "\n",
    "Let $X$ be a real valued random variable. The **distribution function** (also called cumulative distribution function) of $X$, commonly denoted $F_X(x)$ is defined by\n",
    "\n",
    "$$\n",
    "\tF_X(x) = \\Pr(X \\leq x)\n",
    "$$\n",
    "\n",
    "> Properties\n",
    ">\n",
    "- $F$ is monotone non-decreasing\n",
    "- $F$ is right continuous\n",
    "- $\\lim _ {x \\to - \\infty} F(x)=0$ and $\\lim _ {x \\to + \\infty} F(x)=1$\n",
    "\n",
    "The random variables $(X_1, .. , X_n)$ are independent if and only if\n",
    "\n",
    "$$\n",
    "  F _ {(X_1, ... , X_n)} (x) = \\prod _ {i=1}^n F_{X_i} (x_i) \\quad \\forall x \\in \\mathbb R^n\n",
    "$$\n",
    " \n",
    "Let $X$ be a real valued random variable. $X$ has a **probability density function** if there exists $f_X(x)$ such that for all measurable $A \\subset \\mathbb{R}$,\n",
    "\n",
    "$$\n",
    "\t \tP(X \\in A) = \\int_A f_X(x) \\mathrm{d} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **expected value** of a random variable, when it exists, is given by\n",
    "\n",
    "$$\n",
    "\t\t\\mathbb{E}[X] = \\int_ \\Omega X(\\omega) \\mathrm{d} P\n",
    "$$\n",
    "\n",
    "When $X$ has a density, then\n",
    "\n",
    "$$\n",
    "\t\t\\mathbb{E} [X] = \\int_ \\mathbb{R} x f_X (x) \\mathrm{d} x = \\int _ \\mathbb{R} x \\mathrm{d} F_X (x) \n",
    "$$\n",
    "\n",
    "The **empirical expectation** (or **sample average**) is given by\n",
    "\n",
    "$$\n",
    "\t\t\\mathbb{E}_n [z_i] = \\frac{1}{n} \\sum _ {i=1}^N z_i \n",
    "$$\n",
    "\n",
    "The **covariance** of two random variables $X$, $Y$ defined on $\\Omega$ is \n",
    "\n",
    "$$\n",
    "\tCov(X, Y ) = \\mathbb{E}[ (X - \\mathbb{E}[X]) (Y - \\mathbb{E}[Y]) ]  = \\mathbb{E}[XY ] - \\mathbb{E}[X]E[Y]\n",
    "$$\n",
    "\n",
    "In vector notation, $Cov(X, Y) = \\mathbb{E}[XY']  - \\mathbb{E}[X]\\mathbb{E}[Y']$.\n",
    "\n",
    "The **variance** of a random variable $X$, when it exists, is given by\n",
    "\n",
    "$$\n",
    "\tVar(X) = \\mathbb{E}[ (X - \\mathbb{E}[X])^2 ] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n",
    "$$\n",
    "\n",
    "In vector notation,  $Var(X) = \\mathbb{E}[XX'] - \\mathbb{E}[X]\\mathbb{E}[X']$.\n",
    "\n",
    "> Properties\n",
    "> \n",
    "> Let $X, Y, Z, T \\in \\mathcal{L}^{2}$ and $a, b, c, d \\in \\mathbb{R}$\n",
    "> \n",
    ">- $Cov(X, X) = Var(X)$\n",
    ">- $Cov(X, Y) = Cov(Y, X)$\n",
    ">- $Cov(aX + b, Y) = a \\ Cov(X,Y)$\n",
    ">- $Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$\n",
    ">- $Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$\n",
    "\n",
    "Let $X, Y \\in \\mathcal L^1$ be independent. Then, $\\mathbb E[XY] = \\mathbb E[X]E[Y]$.\n",
    "\n",
    "If $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.\n",
    "\n",
    "> Note that the converse does not hold: $Cov(X,Y) = 0 \\not \\to X \\perp Y$.\n",
    "\n",
    "The **sample variance** is given by\n",
    "\n",
    "$$\n",
    "\tVar_n (z_i) = \\frac{1}{n} \\sum _ {i=1}^N (z_i - \\bar{z})^2 \n",
    "$$\n",
    "\n",
    "where $\\bar{z_i} = \\mathbb{E}_n [z_i] = \\frac{1}{n} \\sum _ {i=1}^N z_i$.\n",
    "\n",
    "**Theorem**: \n",
    "The expected sample variance $\\mathbb{E} [\\sigma^2_n] =  \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^N \\left(y_i - \\mathbb{E}_n[y] \\right)^2 \\right]$ gives an estimate of the population variance that is biased by a factor of $\\frac{1}{n}$ and is therefore referred to as **biased sample variance**.\n",
    "\t\n",
    "**Proof**:\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "  &\\mathbb{E}[\\sigma^2_n] =  \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - \\mathbb{E}_n [y] \\right)^2 \\right] = \n",
    "  \\\\\n",
    "  &= \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - \\frac{1}{n} \\sum _ {i=1}^n y_i \\right )^2 \\right] = \n",
    "\t\\\\\n",
    "\t&= \\frac{1}{n} \\sum _ {i=1}^n \\mathbb{E} \\left[ y_i^2 - \\frac{2}{n} y_i \\sum _ {j=1}^n y_j + \\frac{1}{n^2} \\sum _ {j=1}^n y_j \\sum _ {k=1}^{n}y_k  \\right] = \n",
    "\t\\\\\n",
    "\t&= \\frac{1}{n} \\sum _ {i=1}^n  \\left[ \\frac{n-2}{n} \\mathbb{E}[y_i^2]  - \\frac{2}{n} \\sum _ {j\\neq i} \\mathbb{E}[y_i y_j] + \\frac{1}{n^2} \\sum _ {j=1}^n \\sum _ {k\\neq j} \\mathbb{E}[y_j y_k] + \\frac{1}{n^2} \\sum _ {j=1}^n \\mathbb{E}[y_j^2] \\right] = \n",
    "\t\\\\\n",
    "\t&= \\frac{1}{n} \\sum _ {i=1}^n  \\left[ \\frac{n-2}{n}(\\mu^2 + \\sigma^2) - \\frac{2}{n} (n-1) \\mu^2 + \\frac{1}{n^2} n(n-1)\\mu^2 + \\frac{1}{n^2} n (\\mu^2 + \\sigma^2)]\\right] =\n",
    "\t\\\\\n",
    "\t&= \\frac{n-1}{n} \\sigma^2\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\\tag*{$\\blacksquare$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Triangle Inequality**: if $\\mathbb{E}[X] < \\infty$, then \n",
    "\n",
    "$$ |\\mathbb{E} [X] | \\leq \\mathbb{E} [|X|] $$\n",
    " \n",
    "- **Markov's Inequality**: if $\\mathbb{E}[X] < \\infty$, then \n",
    "\n",
    "$$ \\Pr(|X| > t) \\leq \\frac{1}{t} \\mathbb{E}[|X|] $$\n",
    " \n",
    "- **Chebyshev's Inequality**: if $\\mathbb{E}[X^2] < \\infty$,  then\n",
    "\n",
    "$$ \\Pr(|X- \\mu|> t \\sigma) \\leq \\frac{1}{t^2}\\Leftrightarrow \\Pr(|X- \\mu|> t ) \\leq \\frac{\\sigma^2}{t^2} $$\n",
    " \n",
    "- **Cauchy-Schwarz's Inequality**:\n",
    "\n",
    "$$ \\mathbb{E} [|XY|] \\leq \\sqrt{\\mathbb{E}[X^2] \\mathbb{E}[Y^2]} $$\n",
    " \n",
    "- **Minkowski Inequality**: \n",
    "\n",
    "$$ \\left( \\sum _ {k=1}^n | x_k + y_k |^p \\right) ^ {\\frac{1}{p}} \\leq \\left( \\sum _ {k=1}^n | x_k |^p \\right) ^ {\\frac{1}{p}} + \\left( \\sum _ {k=1}^n | y_k | ^p \\right) ^ { \\frac{1}{p} } $$\n",
    " \n",
    "- **Jensen's Inequality**: if $g( \\cdot)$ is concave (e.g. logarithmic function), then \n",
    "\n",
    "$$ \\mathbb{E}[g(x)] \\leq g(\\mathbb{E}[x]) $$ \n",
    " \n",
    " Similarly, if $g(\\cdot)$ is convex (e.g. exponential function), then \n",
    " \n",
    "$$ \\mathbb{E}[g(x)] \\geq g(\\mathbb{E}[x]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Theorems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**: \n",
    "Law of Iterated Expectations\n",
    "\n",
    "$$\n",
    "  \\mathbb{E}(Y) = \\mathbb{E}_X [\\mathbb{E}(Y|X)]\n",
    "$$\n",
    "\n",
    "**Theorem**: \n",
    "Law of Total Variance\n",
    "\n",
    "$$\n",
    "\t\tVar(Y) = Var_X (\\mathbb{E}[Y |X]) + \\mathbb{E}_X [Var(Y|X)] \n",
    "$$\n",
    "\n",
    "Useful distributional results:\n",
    "\n",
    "1. $\\chi^2_q \\sim \\sum _ {i=1}^q Z_i^2$ where $Z_i \\sim N(0,1)$\n",
    "2. $F(n_1 , n_2) \\sim \\frac{\\chi^2 _ {n_1} / n_1}{\\chi^2 _ {n_2}/n_2}$\n",
    "3. $t_n \\sim \\frac{Z}{\\sqrt{\\chi^2 _ n}/n }$\n",
    "\n",
    "> The $t$ distribution is approximately standard normal but has heavier tails. The approximation is good for $n \\geq 30$: $t_{n\\geq 30} \\sim N(0,1)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **statistical model** is a set of probability distributions. More precisely, a **statistical model over data** $D \\in \\mathcal{D}$ is a set of probability distribution over datasets $D$ which takes values in $\\mathcal{D}$.\n",
    "\n",
    "Suppose you have regression data $\\{ \\mathbb{x}_i , y_i \\} _ {i=1}^N$ with $\\mathbb{x}_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$. The statistical model is\n",
    "\n",
    "$$\n",
    " \\Big\\{ P : y_i =  f(\\mathbb{x}_i) + \\varepsilon_i, \\ x_i \\sim F_x , \\ \\varepsilon_i \\sim F _\\varepsilon , \\ \\varepsilon_i \\perp \\mathbb{x}_i , \\ f \\in C^2 (\\mathbb{R}^p) \\Big\\}\n",
    "$$\n",
    "\n",
    "> In words: the statistical model is the set of distributions $P$ such that an additive decomposition of $y_i$ as $f(\\mathbb{x}_i) + \\varepsilon_i$ exists for some $\\mathbb{x}_i$; where $f$ is twice continuously differentiable.\n",
    "\n",
    "A statistical model parameterized by $\\theta \\in \\Theta$ is **well specified** if the data generating process corresponds to some $\\theta_0$ and $\\theta_0 \\in \\Theta$. Otherwise, the statistical model is **misspecified**.\n",
    "\n",
    "A statistical model can be parametrized as $\\mathcal{F} = \\{ P_\\theta \\} _ {\\{ \\theta \\in \\Theta \\}}$. \n",
    "\n",
    "Categories of statistical models:\n",
    "\n",
    "- **Parametric**: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\\{ P_\\theta \\} _ { \\{ \\theta \\in \\Theta \\}}$ with $\\Theta \\subseteq \\mathbb{R}^k, k<\\infty$;\n",
    "- **Semiparametric**: it is a partially specified model, e.g.,  $\\{ P_\\theta \\} _ { \\{ \\theta \\in \\Theta, \\gamma \\in \\Gamma \\}}$ with $\\Theta$ of finite dimension and $\\Gamma$ of infinite dimension;\n",
    "- **Non parametric**: there is no finite dimensional component of the model. \n",
    "\n",
    "In a **linear model data** are given by $D_n = \\{ (y_i, x _ {i1}, \\dots, x _ {ik}) \\} _ {i=1}^n \\in \\mathcal{D}$ where:\n",
    "\n",
    "- $D_n$ are the observed data;\n",
    "- $y_i$ is the dependent variable;\n",
    "- $x_ {i1}, \\dots, x_ {ik}$ are the regressors including a constant.\n",
    "\n",
    "Let $\\mathcal{D}$ be the set of possible data realizations. Let $D \\in \\mathcal{D}$ be your data. Let $\\mathcal{F}$ be a statistical model indexed by $\\theta$. Let $\\nu$ be a functional $\\mathcal{F} \\to \\mathbb{R}$. Let $\\alpha > 0$ be a small tolerance. An **estimator** is a map \n",
    "\n",
    "$$\n",
    "\t\t\\mathcal{D} \\to \\mathcal{F} \\quad , \\quad  D \\mapsto \\hat{\\theta} \\qquad \\text{ or } \\qquad \\mathcal{D} \\to \\mathbb{R} \\quad , \\quad D \\mapsto \\hat{\\nu}\n",
    "$$\n",
    "\n",
    "Statistical **inference** is a map into subsets of $\\mathcal{F}$ given by\n",
    "\n",
    "$$\n",
    "  \\mathcal{D} \\to \\mathcal{G} \\subseteq \\mathcal{F}: \\min _ \\theta P_\\theta (\\mathcal{G} | \\theta \\in \\mathcal{G}) \\geq 1-\\alpha  \\qquad \\text{ or } \\qquad \\mathcal{D} \\to A \\subseteq \\mathbb{R}: \\min _ \\theta P_\\theta (A | \\nu(\\theta) \\in A) \\geq 1-\\alpha\n",
    "$$\n",
    "\n",
    "A **data generating process** (DGP) is a single statistical distribution over $\\mathcal{D}$.\n",
    "\n",
    "Suppose you have a statistical model parametrized by $\\theta$ and an estimator $\\hat{\\theta}$. The **bias** of $\\hat{\\theta}$ relative to $\\theta$ is given by\n",
    "\n",
    "$$\n",
    "\tBias _ {\\theta} (\\hat{\\theta}) = \\mathbb{E} _ {x|\\theta} [\\hat{\\theta} ] - \\theta = \\mathbb{E} _ {x|\\theta} [\\hat{\\theta} - \\theta]\n",
    "$$\n",
    "\n",
    "Let $\\hat{\\theta}$ be an estimator for $\\theta_0$. We say $\\hat{\\theta}$ is an **unbiased** estimator for $\\theta$ if $\\mathbb{E}[\\hat{\\theta}] = \\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kozbur (2019). PhD Econometrics - Lecture Notes.\n",
    "- Greene (2006). \"*Econometric Analysis*\". Appendix B: Probability and Distribution Theory.\n",
    "- Greene (2006). \"*Econometric Analysis*\". Appendix C: Estimation and Inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
