{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [Basics](#Basics)\n",
    " - [Spectral Decomposition](#Spectral-Decomposition)\n",
    " - [Quadratic Forms and Definite Matrices](#Quadratic-Forms-and-Definite-Matrices)\n",
    " - [Matrix Calculus](#Matrix-Calculus)\n",
    " - [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real $n \\times m$ matrix $A$ is an array \n",
    "\n",
    "$$\n",
    "  A=\n",
    "  \\begin{bmatrix}\n",
    "  a_{11} & a_{12} & a_{13} & \\dots  & a_{1m} \\\\\n",
    "  a_{21} & a_{22} & a_{23} & \\dots  & a_{2m} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  a_{n1} & a_{n2} & a_{n3} & \\dots  & a_{nm}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We write $[A]_ {ij} = a_ {ij}$ to indicate the $(i,j)$-element of $A$.\n",
    "\n",
    "> We will usually take the convention that a real vector $x \\in \\mathbb R^n$ is identified with an $n \\times 1$ matrix.\n",
    "\n",
    "The $n \\times n$ **identity matrix** $I_n$ is given by\t\n",
    "\n",
    "$$\n",
    "  [I_n] _ {ij} = \\begin{cases} 1 \\ \\ \\ \\text{if} \\ i=j \\\\\n",
    "  0 \\ \\ \\ \\text{if} \\ i \\neq j \\end{cases}\n",
    "$$\n",
    "\n",
    "Fundamental operations on matrices:\n",
    "\n",
    "1. Two $n \\times m$ matrices, $A,B$, are added element-wise so that $[A+B]_ {ij} = [A]_ {ij} + [B]_ {ij}$.\n",
    "2. A matrix $A$ can be multiplied by a scalar $c\\in \\mathbb{R}$ in which case we set $[cA]_{ij} = c[A]_ {ij}$.  \n",
    "3. An $n \\times m$ matrix $A$ can be multiplied with an $m \\times p$ matrix $B$.  \n",
    "4. The product $AB$ is defined according to the rule $[AB]_{ij} = \\sum_{k=1}^m [A]_{ik}[B]_{kj}$.\n",
    "5. An $n \\times n$ matrix is invertible if there exists a matrix $B$ such that $AB=I$.  In this case, we use the notational convention of writing $B = A^{-1}$. \n",
    "6. Matrix transposition is defined by $[A'] _ {ij} = [A] _ {ji}$.\n",
    "\n",
    "The **trace** of a square matrix $A$ with dimension $n \\times n$ is $\\text{tr}(A) = \\sum _ {i=1}^n a _ {ii}$. \n",
    "\n",
    "The **determinant** of a square $n \\times n$ matrix A is defined according to one of the following three (equivalent) definitions. \n",
    "\n",
    "1. Recursively as $det(A) = \\sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]_{-i,-j})$ where $[A]_{-i,-j}$ is the matrix obtained by deleting the $i$th row and the $j$th column.\n",
    "2. $A \\mapsto det(A)$ under the unique alternating multilinear map on $n \\times n$ matrices such that $I \\mapsto 1$.\n",
    "\n",
    "Vectors $x_1,...,x_k$ are **linearly independent** if the only solution to the equation $b_1x_1 + ... + b_k x_k=0, \\ b_j \\in \\mathbb R$, is $b_1=b_2=...=b_k=0$. \n",
    "\n",
    "Useful matrix identities:\n",
    "\n",
    "- $(A+B)' =A'+B'$\n",
    "- $(AB)C = A(BC)$\n",
    "- $A(B+C) = AB+AC$\n",
    "- $(AB') = B'A'$\n",
    "- $(A^{-1})' = (A')^{-1}$\n",
    "- $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "- $\\text{tr}(cA) = c\\text{tr}(A)$\n",
    "- $\\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B)$\n",
    "- $\\text{tr}(AB) =\\text{tr}(BA)$\n",
    "- $det(I)=1$\n",
    "- $det(cA) = c^ndet(A)$ if  $A$ is $n \\times n$ and $c \\in \\mathbb R$\n",
    "- $det(A) = det(A')$\n",
    "- $det(AB) = det(A)det(B)$\n",
    "- $det(A^{-1}) = (det(A))^{-1}$\n",
    "- $A^{-1}$ exists iff $det(A) \\neq 0$\n",
    "- $rank(A) = rank(A') = rank(A'A) = rank(AA')$\n",
    "- $A^{-1}$ exists iff $rank(A)=n$ for $A$ $n \\times n$\n",
    "- $rank(AB) \\leq \\min \\{ rank(A), rank(B) \\}$ \n",
    "\n",
    "The **rank** of a matrix, $rank(A)$ is equal to the maximal number of linearly independent rows for $A$. \n",
    "\n",
    "Let $A$ be an $n \\times n$ matrix. The $n \\times 1$ vector $x \\neq 0$ is an **eigenvector** of $A$ with corresponding **eigenvalue** $\\lambda$ is $Ax = \\lambda x$.\n",
    "\n",
    "The following types of matrices are defined:\n",
    "\n",
    "1. A matrix $A$ is diagonal if $[A]_ {ij} \\neq 0$ only if $i=j$.\n",
    "2. An $n \\times n$  matrix $A$ is orthogonal if $A'A = I$ \n",
    "3. A matrix $A$ is symmetric if $[A]_ {ij} = [A]_ {ji}$.\n",
    "4. An $n \\times n$ matrix $A$ is idempotent if $A^2=A$.\n",
    "5. The matrix of zeros ($[A]_ {ij} =0$ for each $i,j$) is simply denoted 0.\n",
    "6. An $n \\times n$ matrix $A$ is nilpotent if $A^k=0$ for some integer $k>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "Let $A$ be an $n \\times n$ symmetric matrix.  Then $A$ can be factored as $A = C \\Lambda C'$ where $C$ is orthogonal and $\\Lambda$ is diagonal.\n",
    "\n",
    "If we postmultiply $A$ by $C$, we get\n",
    "\n",
    "- $AC = C \\Lambda C'C$ and\n",
    "- $AC = C \\Lambda$.\t\n",
    "\n",
    "> This is a matrix equation which can be split into columns. The $i$th column of the equation reads $A c_i = \\lambda_i c_i$ which corresponds to the definition of eigenvalues and eigenvectors. So if the decomposition exists, then $C$ is the eigenvector matrix and $\\Lambda$ contains the eigenvalues.\t\n",
    "\n",
    "**Theorem**: \n",
    "The trace of a symmetric matrix equals the sum of its eigenvalues.  The determinant of a symmetric matrix equals the product of its eigenvalues.\n",
    "\n",
    "**Theorem**: \n",
    "The rank of a symmetric matrix equals the number of non zero eigenvalues.\n",
    "\n",
    "**Proof**: \n",
    "$rank(A) = rank(C\\Lambda C') = rank(\\Lambda) = | \\{i: \\lambda_i \\neq 0 \\}|$.\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "**Theorem**: \n",
    "The nonzero eigenvalues of $AA'$ and $A'A$ are identical.\n",
    "\n",
    "**Theorem**: \n",
    "The trace of a symmetric matrix equals the sum of its eignevalues. \n",
    "\n",
    "**Proof**: \n",
    "$tr(A) = tr(C \\Lambda C') = tr((C \\Lambda)C') = tr(C'C \\Lambda) = tr(\\Lambda) = \\sum_ {i=1}^n \\lambda_i.$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "**Theorem**: \n",
    "The determinant of a symmetric matrix equals the product of its eignevalues. \n",
    "\n",
    "**Proof**: \n",
    "$det(A) = det(C \\Lambda C') = det(C)det(\\Lambda)det(C') = det(C)det(C')det(\\Lambda) = det(CC') det(\\Lambda) = det(I)det(\\Lambda) = det(\\Lambda) = \\prod_ {i=1}^n \\lambda_i.$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "**Theorem**: \n",
    "For any symmetric matrix $A$, the eigenvalues of $A^2$ are the square of the eignevalues of $A$, and the eigenvectors are the same.\n",
    "\n",
    "**Proof**: \n",
    "$A = C \\Lambda C' \\implies A^2 = C \\Lambda C' C \\Lambda C' = C \\Lambda I \\Lambda C' = C \\Lambda^2 C'$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "**Theorem**: For any symmetric matrix $A$, and any integer $k>0$, the eigenvalues of $A^k$ are the $k$th power of the eignevalues of $A$, and the eigenvectors are the same. \n",
    "\n",
    "**Theorem**: \n",
    "Any square symmetric matrix $A$ with positive eigenvalues can be written as the product of a lower triangular matrix $L$ and its (upper triangular) transpose $L' = U$. That is $A = LU = LL'$\n",
    "\n",
    "> Note that\n",
    "$$\n",
    "  A = LL' = LU = U'U  = (L')^{-1}L^{-1} = U^{-1}(U')^{-1}\n",
    "$$\n",
    "where $L^{-1}$  is lower triangular and $U^{ -1}$ is upper trianguar. You can check this for the $2 \\times 2$ case. Also note that the validity of the theorem can be extended to symmetric matrices with non- negative eigenvalues by a limiting argument. However, then the proof is not constructive anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Forms and Definite Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **quadratic form** in the $n \\times n$ matrix $A$ and $n \\times 1$ vector $x$ is defined by the scalar $x'Ax$.\n",
    "\n",
    "1. $A$ is negative definite (ND) if for each $x \\neq 0$, $x'Ax < 0$\n",
    "2. $A$ is negative semidefinite (NSD) if for each $x \\neq 0$, $x'Ax \\leq 0$\n",
    "3. $A$ is positive definite (PD) if for each $x \\neq 0$, $x'Ax > 0$\n",
    "4. $A$ is positive semidefinite (PSD) if for each $x \\neq 0$, $x'Ax \\geq 0$\n",
    "\n",
    "**Theorem**: \n",
    "Let $A$ be a symmetric matrix. Then $A$ is PD(ND) $\\iff$ all of its eigenvalues are positive (negative).\n",
    "\n",
    "Some more results:\n",
    "\n",
    "1. If a symmetric matrix $A$ is PD (PSD, ND, NSD), then $\\text{det}(A) >(\\geq,<,\\leq) 0$.\n",
    "2. If symmetric matrix $A$ is PD (ND) then $A^{-1}$  is symmetric PD (ND).\n",
    "3. The identity matrix is PD (since all eigenvalues are equal to 1).\n",
    "4. Every symmetric idempotent matrix is PSD (since the eigenvalues are only 0 or 1). \n",
    "\n",
    "**Theorem**: \n",
    "If $A$ is $n\\times k$ with $n>k$ and $rank(A)=k$, then $A'A$ is PD and $AA'$ is PSD.\n",
    "\n",
    "The **semidefinite partial order** is defined by $A \\geq B$ iff $A-B$ is PSD.\n",
    "\n",
    "**Theorem**: \n",
    "Let $A$, $B$ be symmetric,square , PD, conformable.  Then $A-B$ is PD iff $A^{-1}-B^{-1}$ is PD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define matrices blockwise when they are conformable.  In particular, we assume that if $A_1, A_2, A_3, A_4$ are matrices with appropriate dimensions then the matrix \n",
    "\n",
    "$$\n",
    "\tA = \\begin{bmatrix} A_1 & A_1 \\\\\n",
    "\tA_3 & A_4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is defined in the obvious way. \n",
    "\n",
    "Let $F: \\mathbb R^m \\times \\mathbb R^n \\rightarrow \\mathbb R^p \\times \\mathbb R^q$ be a matrix valued function.  More precisely, given a real  $m \\times n$ matrix $X$, $F(X)$ returns the $p \\times q$ matrix   \n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix}\n",
    "\tf_ {11}(X) & ... & f_ {1q}(X) \\\\ \\vdots & \\ddots & \\vdots \\\\\n",
    "\tf_ {p1}(X)& ... & f_ {pq}(X)\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The derivative of $F$ with respect to the matrix $X$ is the $mp \\times nq$ matrix \n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial F(X)}{\\partial X} = \\begin{bmatrix}\n",
    "\t\\frac{\\partial F(X)}{\\partial x_ {11}} & ... & \\frac{\\partial F(X)}{\\partial x_ {1n}} \\\\ \\vdots & \\ddots & \\vdots \\\\\n",
    "\t\\frac{\\partial F(X)}{\\partial x_ {m1}} & ... & \\frac{\\partial F(X)}{\\partial x_ {mn}}\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each $\\frac{\\partial F(X)}{\\partial x_ {ij}}$ is a $p\\times q$ matrix given by  \n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial F(X)}{\\partial x_ {ij}} = \\begin{bmatrix}\n",
    "\t\\frac{\\partial f_ {11}(X)}{\\partial x_ {ij}} & ... & \\frac{\\partial f_ {1q}(X)}{\\partial x_ {ij}} \\\\\n",
    "\t\\vdots & \\ddots & \\vdots \\\\\n",
    "\t\\frac{\\partial f_ {p1}(X)}{\\partial x_ {ij}} & ... & \\frac{\\partial f_ {pq}(X)}{\\partial x_ {ij}}\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The most important case is when $F: \\mathbb R^n \\rightarrow \\mathbb R$ since this simplifies the derivation of the least squares estimator.  Also, the trickiest thing is to make sure that dimensions are correct. \n",
    "\n",
    "Useful results in matrix calculus:\n",
    "\n",
    "1. $\\frac{\\partial b'x}{\\partial x}= b$ for $dim(b) = dim(x)$\n",
    "2. $\\frac{\\partial B'x}{\\partial x}= B$ for arbitrary, conformable $B$\n",
    "3. $\\frac{\\partial B'x}{\\partial x'}= B'$ for arbitrary, conformable $B$\n",
    "4. $\\frac{\\partial x'Ax}{\\partial x} = (A + A')x$\n",
    "5. $\\frac{\\partial x'Ax}{\\partial A} = xx'$ \n",
    "6. $\\frac{\\partial x'Ax}{\\partial x} = det(A) (A^{-1})'$\n",
    "7. $\\frac{\\partial \\ln det(A)}{\\partial A} = (A^{-1})'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kozbur (2019). PhD Econometrics - Lecture Notes.\n",
    "- Greene (2006). \"*Econometric Analysis*\". Appendix A: Matrix Algebra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
